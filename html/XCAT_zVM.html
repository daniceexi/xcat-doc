<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
</head>
<body>
<div id="TOC">
<ul>
<li><a href="#document-abstract">Document Abstract</a></li>
<li><a href="#terminology">Terminology</a></li>
<li><a href="#support-on-zvm-and-linux-on-system-z">Support on z/VM and Linux on System z</a></li>
<li><a href="#design-architecture">Design Architecture</a></li>
<li><a href="#xcat-setup">xCAT Setup</a></li>
<li><a href="#xcat-commands">xCAT Commands</a></li>
<li><a href="#installing-linux-using-autoyast-or-kickstart">Installing Linux Using AutoYast or Kickstart</a></li>
<li><a href="#installing-linux-using-scsifcp">Installing Linux Using SCSI/FCP</a></li>
<li><a href="#adding-software-packages">Adding Software Packages</a></li>
<li><a href="#cloning-virtual-servers">Cloning Virtual Servers</a></li>
<li><a href="#setting-up-ganglia-on-xcat">Setting Up Ganglia on xCAT</a><ul>
<li><a href="#red-hat-enterprise-linux">Red Hat Enterprise Linux</a></li>
<li><a href="#suse-linux-enterprise-server">SUSE Linux Enterprise Server</a></li>
</ul></li>
<li><a href="#ganglia-monitoring-on-xcat">Ganglia Monitoring on xCAT</a></li>
<li><a href="#statelite">Statelite</a><ul>
<li><a href="#red-hat-enterprise-linux-1">Red Hat Enterprise Linux</a></li>
<li><a href="#suse-linux-enterprise-server-1">SUSE Linux Enterprise Server</a></li>
</ul></li>
<li><a href="#updating-linux">Updating Linux</a></li>
<li><a href="#limitations">Limitations</a></li>
<li><a href="#appendix-a-setting-up-a-second-network">Appendix A: Setting Up a Second Network</a><ul>
<li><a href="#red-hat-enterprise-linux-2">Red Hat Enterprise Linux</a></li>
<li><a href="#suse-linux-enterprise-server-10">SUSE Linux Enterprise Server 10</a></li>
<li><a href="#suse-linux-enterprise-server-11">SUSE Linux Enterprise Server 11</a></li>
</ul></li>
<li><a href="#appendix-b-customizing-autoyast-and-kickstart">Appendix B: Customizing Autoyast and Kickstart</a><ul>
<li><a href="#red-hat-enterprise-server">Red Hat Enterprise Server</a></li>
<li><a href="#suse-linux-enterprise-server-2">SUSE Linux Enterprise Server</a></li>
</ul></li>
<li><a href="#appendix-c-setting-up-network-address-translation">Appendix C: Setting up Network Address Translation</a><ul>
<li><a href="#red-hat-enterprise-server-1">Red Hat Enterprise Server</a></li>
<li><a href="#suse-linux-enterprise-server-3">SUSE Linux Enterprise Server</a></li>
</ul></li>
</ul>
</div>
<p><!-- START doctoc generated TOC please keep comment here to allow auto update --> <!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE --> Table of Contents</p>
<ul>
<li><a href="#document-abstract">Document Abstract</a></li>
<li><a href="#terminology">Terminology</a></li>
<li><a href="#support-on-zvm-and-linux-on-system-z">Support on z/VM and Linux on System z</a></li>
<li><a href="#design-architecture">Design Architecture</a></li>
<li><a href="#xcat-setup">xCAT Setup</a></li>
<li><a href="#xcat-commands">xCAT Commands</a></li>
<li><a href="#installing-linux-using-autoyast-or-kickstart">Installing Linux Using AutoYast or Kickstart</a></li>
<li><a href="#installing-linux-using-scsifcp">Installing Linux Using SCSI/FCP</a></li>
<li><a href="#adding-software-packages">Adding Software Packages</a></li>
<li><a href="#cloning-virtual-servers">Cloning Virtual Servers</a></li>
<li><a href="#setting-up-ganglia-on-xcat">Setting Up Ganglia on xCAT</a></li>
<li><a href="#red-hat-enterprise-linux">Red Hat Enterprise Linux</a></li>
<li><a href="#suse-linux-enterprise-server">SUSE Linux Enterprise Server</a></li>
<li><a href="#ganglia-monitoring-on-xcat">Ganglia Monitoring on xCAT</a></li>
<li><a href="#statelite">Statelite</a></li>
<li><a href="#red-hat-enterprise-linux-1">Red Hat Enterprise Linux</a></li>
<li><a href="#suse-linux-enterprise-server-1">SUSE Linux Enterprise Server</a></li>
<li><a href="#updating-linux">Updating Linux</a></li>
<li><a href="#limitations">Limitations</a></li>
<li><a href="#appendix-a-setting-up-a-second-network">Appendix A: Setting Up a Second Network</a></li>
<li><a href="#red-hat-enterprise-linux-2">Red Hat Enterprise Linux</a></li>
<li><a href="#suse-linux-enterprise-server-10">SUSE Linux Enterprise Server 10</a></li>
<li><a href="#suse-linux-enterprise-server-11">SUSE Linux Enterprise Server 11</a></li>
<li><a href="#appendix-b-customizing-autoyast-and-kickstart">Appendix B: Customizing Autoyast and Kickstart</a></li>
<li><a href="#red-hat-enterprise-server">Red Hat Enterprise Server</a></li>
<li><a href="#suse-linux-enterprise-server-2">SUSE Linux Enterprise Server</a></li>
<li><a href="#appendix-c-setting-up-network-address-translation">Appendix C: Setting up Network Address Translation</a></li>
<li><a href="#red-hat-enterprise-server-1">Red Hat Enterprise Server</a></li>
<li><a href="#suse-linux-enterprise-server-3">SUSE Linux Enterprise Server</a></li>
</ul>
<!-- END doctoc generated TOC please keep comment here to allow auto update -->

<div class="figure">
<img src="http://sourceforge.net/p/xcat/wiki/XCAT_Documentation/attachment/Official-xcat-doc.png" /><p class="caption"></p>
</div>
<h2 id="document-abstract"><a href="#TOC">Document Abstract</a></h2>
<p>This document provides an overview and a quick start guide on basic z/VM and Linux on System z administration using xCAT. For technical support, please post your question(s) on the <a href="https://lists.sourceforge.net/lists/listinfo/xcat-user">mailing-list</a>.</p>
<h2 id="terminology"><a href="#TOC">Terminology</a></h2>
<p>This section outlines the terminology used within this document.</p>
<ul>
<li><strong>DirMaint</strong>: CMS application that helps manage an installation's VM directory.</li>
<li><strong>Ganglia</strong>: <em>&quot;Ganglia consists of two unique daemons (gmond and gmetad), a PHP-based web frontend and a few other small utility programs. Gmond is a multi-threaded daemon which runs on each cluster node you want to monitor. Gmetad is the daemon that monitors the other nodes by periodically polling them, parsing the collected XML, and saving all the numeric, volatile metrics to the round-robin databases.&quot;</em> - Ganglia Development Team</li>
<li><strong>Life cycle</strong>: A collection of tasks that include: power on/off of a virtual server, and create/edit/delete of a virtual server.</li>
<li><strong>SMAPI</strong>: The Systems Management APIs simplify the task of managing many virtual images running under a single z/VM image.</li>
<li><strong>Virtual server</strong>: A server composed of virtualized resources. An operating system can be installed on a virtual server.</li>
<li><strong>VMCP</strong>: Linux module that allows execution of CP commands.</li>
<li>CP: <em>&quot;The Control Program (CP) is the operating system that underlies all of z/VM. It is responsible for virtualizing your z/Series machine's real hardware, and allowing many virtual machines to simultaneously share the hardware resource.&quot;</em> - IBM</li>
<li><strong>xCAT</strong>: xCAT (Extreme Cloud Administration Tool) is a toolkit that provides support for the deployment and administration of large cloud environments.</li>
<li><strong>zHCP</strong>: zHCP (System z Hardware control point) is a Linux virtual server that interfaces with SMAPI and CP and manages other virtual servers on z/VM.</li>
<li><strong>AutoYaST</strong>: <em>&quot;AutoYaST is a system for installing one or more SUSE Linux systems automatically and without user intervention. AutoYaST installations are performed using an autoyast profile with installation and configuration data.&quot;</em> -SUSE</li>
<li><strong>Kickstart</strong>: <em>&quot;Automated installation for Red Hat. It uses a file containing the answers to all the questions that would normally be asked during a typical Red Hat Linux installation.&quot;</em> -Red Hat</li>
</ul>
<h2 id="support-on-zvm-and-linux-on-system-z"><a href="#TOC">Support on z/VM and Linux on System z</a></h2>
<p>This section provides a list of supported functionalities on xCAT for z/VM and Linux on System z.</p>
<ol style="list-style-type: decimal">
<li>Lifecycle Management
<ul>
<li>Power on/off VM</li>
<li>Create/edit/delete VM</li>
<li>Migrate VM between any z/VM in an SSI cluster (only in z/VM 6.2)</li>
</ul></li>
<li>Inventory
<ul>
<li>Software and hardware inventory of VM or z/VM system</li>
<li>Resource (e.g. disks, networks) inventory of z/VM system</li>
</ul></li>
<li>Image Management
<ul>
<li>Cloning VM</li>
<li>Vanilla installation of Linux via Autoyast or Kickstart</li>
<li>Provisioning diskless VM via NFS read-only root filesystem</li>
</ul></li>
<li>Network Management
<ul>
<li>Supports Layer 2 and 3 network switching for QDIO GLAN/VSWITCH and Hipersockets GLAN</li>
<li>Create/edit/delete QDIO GLAN/VSWITCH and Hipersockets GLAN</li>
<li>Add/delete virtual network devices to VM</li>
</ul></li>
<li>Storage Management
<ul>
<li>Manage ECKD/FBAnative SCSI disks within a disk pool</li>
<li>Add/remove ECKD/FBA/native SCSI disks from VM</li>
<li>Attach or detach ECKD/FBA/native SCSI disks to a z/VM system</li>
</ul></li>
<li>OS Management
<ul>
<li>Upgrading Linux OS</li>
<li>Add/update/remove software packages on OS</li>
<li>Basic xCAT functionalities, e.g. remote shell, post-scripts, rsync, etc.</li>
</ul></li>
<li>Monitoring
<ul>
<li>Linux monitoring using Ganglia</li>
</ul></li>
<li>Others
<ul>
<li>Full command line interface support</li>
<li>Web user interface support</li>
<li>Self-service portal to provision VM on demand</li>
</ul></li>
</ol>
<h2 id="design-architecture"><a href="#TOC">Design Architecture</a></h2>
<p>This section provides an architectural overview of xCAT on z/VM and Linux on System z.</p>
<p>[[img src=Architecture.png]] <strong>Figure 1</strong>. Shows the layout of xCAT on System z.</p>
<p>xCAT can be used to manage virtual servers spanning across multiple z/VM partitions. The xCAT management node (MN) runs on any Linux virtual server. It manages each z/VM partition using a System z hardware control point (zHCP) running on a privileged Linux virtual server. The zHCP interfaces with z/VM systems management API (SMAPI), directory manager (DirMaint), and control program layer (CP) to manage the z/VM partition. It utilizes a C socket interface to communicate with the SMAPI layer and VMCP Linux module to communicate with the CP layer.</p>
<h2 id="xcat-setup"><a href="#TOC">xCAT Setup</a></h2>
<p>Before continuing, you should have gone through the <a href="https://sourceforge.net/apps/mediawiki/xcat/index.php?title=XCAT_zVM_Setup">xCAT Setup on zVM and Linux on System z</a>.</p>
<h2 id="xcat-commands"><a href="#TOC">xCAT Commands</a></h2>
<p>This section lists the current xCAT commands supported on z/VM and Linux on System z.</p>
<p><code>rpower</code> - Controls the power for a node or noderange.<br />The syntax is: <code>rpower &amp;lt;node&amp;gt; [on|off|softoff|stat|reset|reboot|pause|unpause]</code></p>
<pre><code># rpower gpok3 stat
gpok3: on</code></pre>
<p>Note: You should cleanly shutdown the node by issuing <code>rpower &amp;lt;node&amp;gt; softoff</code>.</p>
<p><code>mkvm</code> - Creates a new virtual server with the same profile/resources as the specified node (cloning). Alternatively, creates a new virtual server based on a directory entry.<br />The syntax is: <code>mkvm &amp;lt;new node&amp;gt; /tmp/&amp;lt;directory_entry_text_file&amp;gt;</code></p>
<pre><code># mkvm gpok3 /tmp/dirEntry.txt
gpok3: Creating user directory entry for LNX3... Done</code></pre>
<p>The directory entry can also be piped into stdin, using <code>cat</code> or <code>echo</code>.</p>
<pre><code>cat /tmp/dirEntry.txt | mkvm gpok3 -s
gpok3: Creating user directory entry for LNX3... Done</code></pre>
<p>For cloning, the syntax is: <code>mkvm &amp;lt;target Linux&amp;gt; &amp;lt;source Linux&amp;gt; pool=&amp;lt;disk pool&amp;gt;</code></p>
<pre><code># mkvm gpok4 gpok3 pool=POOL1
gpok4: Cloning gpok3
gpok4: Linking source disk (0100) as (1100)
gpok4: Linking source disk (0101) as (1101)
gpok4: Stopping LNX3... Done
gpok4: Creating user directory entry
gpok4: Granting VSwitch (VSW1) access for gpok3
gpok4: Granting VSwitch (VSW2) access for gpok3
gpok4: Adding minidisk (0100)
gpok4: Adding minidisk (0101)
gpok4: Disks added (2). Disks in user entry (2)
gpok4: Linking target disk (0100) as (2100)
gpok4: Copying source disk (1100) to target disk (2100) using FLASHCOPY
gpok4: Mounting /dev/dasdg1 to /mnt/LNX3
gpok4: Setting network configuration
gpok4: Linking target disk (0101) as (2101)
gpok4: Copying source disk (1101) to target disk (2101) using FLASHCOPY
gpok4: Powering on
gpok4: Detatching source disk (0101) at (1101)
gpok4: Detatching source disk (0100) at (1100)
gpok4: Starting LNX3... Done</code></pre>
<p><code>rmvm</code> - Removes a virtual server.<br />The syntax is: <code>rmvm &amp;lt;node&amp;gt;</code>.</p>
<pre><code># rmvm gpok3
gpok3: Deleting virtual server LNX3... Done</code></pre>
<p><code>lstree</code> - Display VM hierarchy. It is important to run <code>rscan -w &amp;lt;hcp&amp;gt;</code> to populate the <code>zvm</code> table before trying this command.<br />The syntax is: <code>lstree &amp;lt;hcp&amp;gt;</code>.</p>
<pre><code>CEC: 1ABCD
|__LPAR: MNO1
   |__zVM: POKDEV61
      |__VM: gpok2 (LNX2)
      |__VM: gpok3 (LNX3)
      |__VM: gpok4 (LNX4)
      |__VM: gpok5 (LNX5)</code></pre>
<p><code>lsvm</code> - List a virtual server's configuration. Options supported are:</p>
<ul>
<li><p>Show the directory entry.<br />The syntax is: <code>lsvm &amp;lt;node&amp;gt;</code></p>
<pre><code># lsvm gpok3
gpok3: USER LNX3 PWD 512M 1G G
gpok3: INCLUDE LNXDFLT
gpok3: COMMAND SET VSWITCH VSW2 GRANT LNX3</code></pre></li>
<li><p>List the defined network names available for a given node.<br />The syntax is: <code>lsvm &amp;lt;node&amp;gt; --getnetworknames</code></p>
<pre><code># lsvm gpok3 --getnetworknames
gpok3: LAN:QDIO SYSTEM GLAN1
gpok3: LAN:HIPERS SYSTEM GLAN2
gpok3: LAN:QDIO SYSTEM GLAN3
gpok3: VSWITCH SYSTEM VLANTST1
gpok3: VSWITCH SYSTEM VLANTST2
gpok3: VSWITCH SYSTEM VSW1
gpok3: VSWITCH SYSTEM VSW2
gpok3: VSWITCH SYSTEM VSW3</code></pre></li>
<li><p>List the configuration for a given network.<br />The syntax is: <code>lsvm &amp;lt;node&amp;gt; --getnetwork [network_name]</code></p>
<pre><code># lsvm gpok3 --getnetwork GLAN1
gpok3: LAN SYSTEM GLAN1        Type: QDIO    Connected: 1    Maxconn: INFINITE
gpok3:   PERSISTENT  UNRESTRICTED  IP                        Accounting: OFF
gpok3:   IPTimeout: 5                 MAC Protection: Unspecified
gpok3:   Isolation Status: OFF</code></pre></li>
<li><p>List the disk pool names available.<br />The syntax is: <code>lsvm &amp;lt;node&amp;gt; --diskpoolnames</code></p>
<pre><code># lsvm gpok3 --diskpoolnames
gpok3: POOL1
gpok3: POOL2
gpok3: POOL3</code></pre></li>
<li><p>List the configuration for a given disk pool.<br />The syntax is: <code>lsvm &amp;lt;node&amp;gt; --diskpool [pool_name] [space (free or used)]</code></p>
<pre><code># lsvm gpok3 --diskpool POOL1 free
gpok3: #VolID DevType StartAddr Size
gpok3: EMC2C4 3390-09 0001 10016
gpok3: EMC2C5 3390-09 0001 10016</code></pre></li>
</ul>
<p><code>chhypervisor</code> - Changes the z/VM hypervisor's configuration. This command is only available in the development build of xCAT at this time.<br />Note this command will work only if the z/VM hypervisor definition is created:</p>
<pre><code># nodeadd pokdev61 groups=hosts hypervisor.type=zvm nodetype.os=zvm6.1 zvm.hcp=gpok2.endicott.ibm.com mgt=zvm</code></pre>
<p>Here, <code>pokdev61</code> is the z/VM system name, <code>zvm6.1</code> is the z/VM version, and <code>gpok2.endicott.ibm.com</code> is the full domain name of the zHCP.</p>
<p>Options supported are:</p>
<ul>
<li><p>Add a disk to a disk pool defined in the EXTENT CONTROL. The disk has to already be attached to SYSTEM and formatted using CPFMTXA or CPFORMAT.<br />The syntax is: <code>chhypervisor &amp;lt;node&amp;gt; --adddisk2pool [function] [region] [volume] [group]</code>. Function type can be either: (4) Define region as full volume and add to group OR (5) Add existing region to group. If the volume already exists in the EXTENT CONTROL, use function 5. If the volume does not exist in the EXTENT CONTROL, but is attached to SYSTEM, use function 4.</p>
<pre><code># chhypervisor pokdev61 --adddisk2pool 4 DM1234 DM1234 POOL1
gpok2: Adding DM1234 to POOL1... Done



# chhypervisor pokdev61 --adddisk2pool 5 DM1234 POOL1
gpok2: Adding DM1234 to POOL1... Done</code></pre></li>
<li><p>Dynamically add an ECKD disk to a running z/VM system.<br />The syntax is: <code>chhypervisor &amp;lt;node&amp;gt; --addeckd [device_number]</code></p>
<pre><code># chhypervisor pokdev61 --addeckd DM1234
gpok2: Adding ECKD disk to system... Done</code></pre></li>
<li>Dynamically add a SCSI disk to a running z/VM system. The SCSI disk is added to the system as an EDEV.<br />The syntax is: <code>chhypervisor &amp;lt;node&amp;gt; --addscsi [device_number] [device_path] [option] [persist]</code>.
<ul>
<li><code>device_number</code> is the device number.</li>
<li><code>device_path</code> is a comma separated string containing the FCP device number, WWPN, and LUN.</li>
<li><code>option</code> can be: (1) add new SCSI (default), (2) add new path, or (3) delete path.</li>
<li><p><code>persist</code> can be: (YES) SCSI device updated in active and configured system, or (NO) SCSI device updated only in active system.</p>
<h1><a href="#TOC">chhypervisor pokdev61 --addscsi 9000 &quot;1A23,500512345678c411,4012345100000000;1A89,500512345678c411,4012345200000000&quot; 2 YES</a></h1>
<p>gpok2: Adding a real SCSI disk to system... Done</p></li>
</ul></li>
<li>Create a virtual network LAN<br />The syntax is: <code>chhypervisor &amp;lt;node&amp;gt; --addvlan [name] [owner] [type] [transport]</code>.
<ul>
<li><code>type</code> must be: (1) unrestricted simulated HiperSockets NIC, (2) unrestricted simulated QDIO NIC, (3) restricted simulated HiperSockets NIC, or (4) restricted simulated QDIO NIC.</li>
<li><p><code>transport</code> must be: (0) unspecified, (1) IP, or (2) ethernet.</p>
<h1><a href="#TOC">chhypervisor pokdev61 --addvlan GLAN1 SYSTEM 2 2</a></h1>
<p>gpok2: Creating virtual network LAN GLAN1... Done</p></li>
</ul></li>
<li>Create a virtual switch<br />The syntax is: <code>chhypervisor &amp;lt;node&amp;gt; --addvswitch [name] [osa_device_address] [port_name] [controller] [connect (0, 1, or 2)] [memory_queue] [router] [transport] [vlan_id] [port_type] [update] [gvrp] [native vlan]</code>.
<ul>
<li><code>name</code> is the name of the virtual switch.</li>
<li><code>osa_device_address</code> is the real device address of a real OSA-Express QDIO device used to create the switch.</li>
<li><code>port_name</code> is name used to identify the OSA Expanded adapter.</li>
<li><code>port_name</code> is name used to identify the OSA Expanded adapter.</li>
<li><code>controller</code> is the user Id controlling the real device.</li>
<li>(Optional) <code>memory_queue</code> is the QDIO buffer size in megabytes.</li>
<li>(Optional) <code>router</code> Specifies whether the OSA-Express QDIO device will act as a router to the virtual switch and it can be: (0) unspecified, (1) not a router, or (2) primary router.</li>
<li>(Optional) <code>transport</code> specifies the transport mechanism to be used and it can be: (0) unspecified, (1) IP, or (2) ethernet.</li>
<li>(Optional) <code>vlan_id</code> is the VLAN ID.</li>
<li>(Optional) <code>port_type</code> is the port type and it can be: (0) unspecified, (1) access, or (2) trunk.</li>
<li>(Optional) <code>update</code> can be: (0) unspecified, (1) create a virtual switch on the active system, (2) create a virtual switch on the active system and add the definition to the system configuration file, or (3) add the virtual switch definition to the system configuration file.</li>
<li>(Optional) <code>gvrp</code> can be: (0) unspecified, (1) GVRP, or (2) no-GVRP.</li>
<li><p>(Optional) <code>native_vlan</code> is the native VLAN.</p>
<h1><a href="#TOC">chhypervisor pokdev61 --addvswitch VSW1 8050 FOOBAR DTCVSW1</a></h1>
<p>gpok2: Creating virtual switch VSW1... Done</p></li>
</ul></li>
<li><p>Add a zFCP device to a device pool defined in xCAT. The device must have been carved up in the storage controller and configured with a WWPN/LUN before it can be added to the xCAT storage pool. z/VM does not have the ability to communicate directly with the storage controller to carve up disks dynamically.<br />The syntax is: <code>chhypervisor &amp;lt;node&amp;gt; --addzfcp2pool [pool] [state (free or used)] [wwpn] [lun] [size] [range (optional)] [owner (optional)]</code>. Multiple WWPNs can be specified for the same LUN (multi-pathing), each separated with a semi-colon.</p>
<pre><code># chhypervisor pokdev61 --addzfcp2pool zfcp1 free 500501234567C890 4012345600000000 8G
pokdev61: Adding zFCP device to zfcp1 pool... Done</code></pre></li>
<li><p>Remove a disk from a disk pool defined in the EXTENT CONTROL.<br />The syntax is: <code>chhypervisor &amp;lt;node&amp;gt; --removediskfrompool [function] [region] [group]</code>. Function type can be either: (1) Remove region, (2) Remove region from group, (3) Remove region from all groups, OR (7) Remove entire group</p></li>
</ul>
<p>Remove a region from the EXTENT CONTROL:</p>
<pre><code>    # chhypervisor pokdev61 --removediskfrompool 1 DM1234
    gpok2: Removing DM1234... Done</code></pre>
<p>Remove a region from a group in the EXTENT CONTROL:</p>
<pre><code>    # chhypervisor pokdev61 --removediskfrompool 2 DM1234 POOL1
    gpok2: Removing DM1234 from POOL1... Done</code></pre>
<p>Remove a region from all groups in the EXTENT CONTROL:</p>
<pre><code>    # chhypervisor pokdev61 --removediskfrompool 3 DM1234
    gpok2: Removing DM1234... Done</code></pre>
<p>Remove group POOL1 in the EXTENT CONTROL (The second argument has no significance):</p>
<pre><code>    # chhypervisor pokdev61 --removediskfrompool 7 FOOBAR POOL1
    gpok2: Removing POOL1... Done</code></pre>
<ul>
<li>Delete a real SCSI disk (EDEV).<br />The syntax is: <code>chhypervisor &amp;lt;node&amp;gt; --removescsi [device number] [persist (YES or NO)]</code>.
<ul>
<li><p><code>persist</code> can be: (NO) SCSI device is deleted on the active system, or (YES) SCSI device is deleted from the active system and permanent configuration for the system.</p>
<h1><a href="#TOC">chhypervisor pokdev61 --removescsi 9000 YES</a></h1>
<p>pokdev61: Deleting a real SCSI disk for system... Done</p></li>
</ul></li>
<li><p>Delete a virtual network LAN.<br />The syntax is: <code>chhypervisor &amp;lt;node&amp;gt; --removevlan [name] [owner]</code>.</p>
<pre><code># chhypervisor pokdev61 --removevlan GLAN1 SYSTEM
pokdev61: Deleting virtual network LAN GLAN1... Done</code></pre></li>
<li><p>Delete a virtual switch.<br />The syntax is: <code>chhypervisor &amp;lt;node&amp;gt; --removevswitch [name]</code>.</p>
<pre><code># chhypervisor pokdev61 --removevswitch VSW1
pokdev61: Deleting virtual switch VSW1... Done</code></pre></li>
<li><p>Remove a zFCP device from a device pool defined in xCAT.<br />The syntax is: <code>chhypervisor &amp;lt;node&amp;gt; --removezfcpfrompool [pool] [lun] [wwpn (optional)]</code></p>
<pre><code># chhypervisor pokdev61 --removezfcpfrompool zfcp1 4012345600000000
pokdev61: Removing zFCP device 4012345600000000 from zfcp1 pool... Done</code></pre></li>
<li><p>Execute a SMAPI function.<br />The syntax is: <code>chhypervisor &amp;lt;node&amp;gt; --smcli [function_name] [args]</code></p>
<pre><code># chhypervisor pokdev62 --smcli Image_Query_DM -T LNX3
pokdev61: USER LNX3 PSWD 796M 1G G
pokdev61: INCLUDE LNXDFLT
pokdev61: COMMAND SET VSWITCH VSW2 GRANT LNX3
pokdev61: MDISK 0100 3390 0001 10016 EMC123 MR
pokdev61: *DVHOPT LNK0 LOG1 RCM1 SMS0 NPW1 LNGAMENG PWC20121011 CRC??</code></pre></li>
</ul>
<p>A list of APIs supported can be found by using the help flag, e.g. <code>chhypervisor pokdev62 --smcli -h</code>. Specific arguments associated with a SMAPI function can be found by using the help flag for the function, e.g. <code>chhypervisor pokdev62 --smcli Image_Query_DM -h</code>. Only <a href="http://publib.boulder.ibm.com/infocenter/zvm/v6r2/topic/com.ibm.zvm.v620.dmse6/toc.htm">z/VM 6.2 SMAPI functions</a> or older are supported at this time. Additional SMAPI functions will be added in subsequent zHCP versions.</p>
<p>If an API is not supported in the level of SMAPI, you will receive:</p>
<pre><code>      Return Code: 900
      Reason Code: 12
      Description: Specified function does not exist</code></pre>
<p><code>chvm</code> - Changes the virtual machine's configuration. Note some option specifics are only in the development build. Options supported are:</p>
<ul>
<li><p>Adds a 3390 (ECKD) disk to a virtual machine's directory entry.<br />The syntax is: <code>chvm &amp;lt;node&amp;gt; --add3390 [disk pool] [device address (or auto)] [size (G, M, or cylinders)] [mode] [read password (optional)] [write password (optional)] [multi password (optional)]</code>. If 'auto' is specified in place of a device address, xCAT will find a freely available device address.</p>
<pre><code># chvm gpok3 --add3390 POOL1 0101 3338 MR
gpok3: Adding disk 0101 to LNX3... Done

# chvm gpok3 --add3390 POOL1 0102 2G MR
gpok3: Adding disk 0102 to LNX3... Done</code></pre></li>
<li><p>Adds a 3390 (ECKD) disk that is defined in a virtual machine's directory entry to that virtual machine's active configuration.<br />The syntax is: <code>chvm &amp;lt;node&amp;gt; --add3390active [device address] [mode]</code></p>
<pre><code># chvm gpok3 --add3390active 0101 MR
gpok3: Adding disk 0101 to LNX3... Done</code></pre></li>
<li><p>Adds a 9336 (FBA) disk to a virtual machine's directory entry.<br />The syntax is: <code>chvm &amp;lt;node&amp;gt; --add9336 [disk pool] [virtual device (or auto)] [size (blocks)] [mode] [read password (optional)] [write password (optional)] [multi password (optional)]</code>. If 'auto' is specified in place of a device address, xCAT will find a freely available device address.</p>
<pre><code># chvm gpok3 --add9336 POOL3 0101 4194272 MR
gpok3: Adding disk 0101 to LNX3... Done

# chvm gpok3 --add9336 POOL3 0102 6G MR
gpok3: Adding disk 0102 to LNX3... Done</code></pre></li>
<li><p>Adds a network adapter to a virtual machine's directory entry (case sensitive).<br />The syntax is: <code>chvm &amp;lt;node&amp;gt; --addnic [address] [type] [device count]</code></p>
<pre><code># chvm gpok3 --addnic 0600 QDIO 3
gpok3: Adding NIC 0900 to LNX3... Done</code></pre></li>
<li><p>Adds a virtual processor to a virtual machine's directory entry.<br />The syntax is: <code>chvm &amp;lt;node&amp;gt; --addprocessor [address]</code></p>
<pre><code># chvm gpok3 --addprocessor 01
gpok3: Adding processor 01 to LNX3... Done</code></pre></li>
<li><p>Adds a virtual processor to a virtual machine's active configuration (case sensitive).<br />The syntax is: <code>chvm &amp;lt;node&amp;gt; --addprocessoractive [address] [type]</code></p>
<pre><code># chvm gpok3 --addprocessoractive 01 IFL
gpok3: CPU 01 defined</code></pre></li>
<li><p>Adds a v-disk to a virtual machine's directory entry.<br />The syntax is: <code>chvm &amp;lt;node&amp;gt; --addvdisk [device address] [size] [mode]</code></p>
<pre><code># chvm gpok3 --addvdisk 0300 2097120 MR
gpok3: Adding V-Disk 0300 to LNX3... Done</code></pre></li>
<li><p>Adds a zFCP device to a virtual machine. This command is only available in the development build of xCAT at this time.<br />The syntax is: <code>chvm &amp;lt;node&amp;gt; --addzfcp [pool] [device_address] [loaddev (0 or 1)] [size] [tag (optional)] [wwpn (optional)] [lun (optional)]</code>.<br />The device address must be a dedicated FCP channel attached to the virtual server. Use <code>dedicatedevice</code> option to dedicate the FCP channel. The loaddev option allows the virtual server to boot from the zFCP device that is to be attached.</p>
<pre><code># chvm gpok3 --addzfcp zfcp1 b15a 0 2g
gpok3: Using device with WWPN/LUN of 500501234567C890/4012345600000000
gpok3: Configuring FCP device to be persistent... Done
gpok3: Adding FCP device... Done</code></pre></li>
<li><p>Connects a given network adapter to a GuestLAN.<br />The syntax is: <code>chvm &amp;lt;node&amp;gt; --connectnic2guestlan [address] [lan] [owner]</code></p>
<pre><code># chvm gpok3 --connectnic2guestlan 0600 GLAN1 LN1OWNR
gpok3: Connecting NIC 0600 to GuestLan GLAN1 on LN1OWNR... Done</code></pre></li>
<li><p>Connects a given network adapter to a vSwitch.<br />The syntax is: <code>chvm &amp;lt;node&amp;gt; --connectnic2vswitch [address] [vswitch]</code></p>
<pre><code># chvm gpok3 --connectnic2vswitch 0600 VSW1
gpok3: Connecting NIC 0600 to VSwitch VSW1 on LNX3... Done</code></pre></li>
<li>Copy a disk attached to a given virtual machine.<br />The syntax is: <code>chvm &amp;lt;node&amp;gt; --copydisk [target_address] [source_node] [source_address]</code>
<ul>
<li><code>target_address</code> is the virtual address of the disk you are going to copy into.</li>
<li><code>source_node</code> is the node where the source disk resides.</li>
<li><p><code>source_address</code> is the virtual address of the disk you are going to copy from.</p>
<h1><a href="#TOC">chvm gpok3 --copydisk 0100 gpok2 0101</a></h1></li>
</ul></li>
<li><p>Adds a dedicated device to a virtual machine's directory entry.<br />The syntax is: <code>chvm &amp;lt;node&amp;gt; --dedicatedevice [virtual device] [real device] [read-only (0 or 1)]</code>. Specify 1 for <code>read-only</code> if the virtual device is to be in read-only mode, otherwise, specify a 0.</p>
<pre><code># chvm gpok3 --dedicatedevice 0101 637F 0
gpok3: Dedicating device 637F as 0101 to LNX3... Done</code></pre></li>
<li><p>Deletes the IPL statement from the virtual machine's directory entry.<br />The syntax is: <code>chvm &amp;lt;node&amp;gt; --deleteipl</code></p>
<pre><code># chvm gpok3 --deleteipl
gpok3: Removing IPL statement on LNX3... Done</code></pre></li>
<li><p>Disconnects a given network adapter.<br />The syntax is: <code>chvm &amp;lt;node&amp;gt; --disconnectnic [address]</code></p>
<pre><code># chvm gpok3 --disconnectnic 0600
gpok3: Disconnecting NIC 0600 on LNX3... Done</code></pre></li>
<li><p>Formats a disk attached to a given virtual machine using dasdfmt (only ECKD disks supported). The disk should not be linked to any other virtual server, and the virtual server should be powered off. This command is best used after add3390().<br />The syntax is: <code>chvm &amp;lt;node&amp;gt; --formatdisk [disk address] [multi password (optional)]</code></p>
<pre><code># chvm gpok3 --formatdisk 0100
gpok3: Linking target disk (0100) as (1100)
gpok3: Formating target disk (dasdg)
gpok3: Detatching target disk (1100)
gpok3: Done</code></pre></li>
<li><p>Grant vSwitch access for given virtual machine.<br />The syntax is: <code>chvm &amp;lt;node&amp;gt; --grantvswitch [vSwitch]</code></p>
<pre><code># chvm gpok3 --grantvswitch VSW1
gpok3: Granting VSwitch (VSW1) access for LNX3... Done</code></pre></li>
<li><p>Purge the reader contents of a virtual machine.<br />The syntax is: <code>chvm &amp;lt;node&amp;gt; --purgerdr</code></p>
<pre><code># chvm gpok3 --purgerdr
gpok3: Purging reader contents of LNX3</code></pre></li>
<li><p>Removes a minidisk from a virtual machine's directory entry.<br />The syntax is: <code>chvm &amp;lt;node&amp;gt; --removedisk [virtual device]</code></p>
<pre><code># chvm gpok3 --removedisk 0101
gpok3: Removing disk 0101 on LNX3... Done</code></pre></li>
<li><p>Removes a network adapter from a virtual machine's directory entry.<br />The syntax is: <code>chvm &amp;lt;node&amp;gt; --removenic [address]</code></p>
<pre><code># chvm gpok3 --removenic 0700
gpok3: Removing NIC 0700 on LNX3... Done</code></pre></li>
<li><p>Removes a processor from an active virtual machine's configuration.<br />The syntax is: <code>chvm &amp;lt;node&amp;gt; --removeprocessor [address]</code></p>
<pre><code># chvm gpok3 --removeprocessor 01
gpok3: Removing processor 01 on LNX3... Done</code></pre></li>
<li><p>Removes the LOADDEV statement from a virtual machines's directory entry.<br />The syntax is: <code>chvm &amp;lt;node&amp;gt; --removeloaddev [wwpn] [lun]</code></p>
<pre><code># chvm gpok3 --removeloaddev 500501234567C890 4012345600000000
gpok3: Removing LOADDEV directory statements
gpok3: Replacing user entry of LNX3... Done</code></pre></li>
<li><p>Removes a zFCP device from a virtual machine. This command is only available in the development build of xCAT at this time.<br />The syntax is: <code>chvm &amp;lt;node&amp;gt; --removezfcp [device address] [wwpn] [lun]</code></p>
<pre><code># chvm gpok3 --removezfcp b15a 500501234567C890 4012345600000000
gpok3: Updating FCP device pool... Done
gpok3: De-configuring FCP device on host... Done</code></pre></li>
<li><p>Replaces a virtual server's directory entry.<br />The syntax is: <code>chvm &amp;lt;node&amp;gt; --replacevs [directory entry]</code>. The directory entry can also be piped into stdin, using <code>cat</code> or <code>echo</code>.</p>
<pre><code># cat /tmp/dirEntry.txt | chvm gpok3 --replacevs 
gpok3: Replacing user entry of LNX3... Done</code></pre></li>
<li><p>Sets the IPL statement for a given virtual server.<br />The syntax is: <code>chvm &amp;lt;node&amp;gt; --setipl [ipl target] [load parms] [parms]</code></p>
<pre><code># chvm gpok3 --setipl CMS
gpok3: Setting IPL statement on LNX3... Done</code></pre></li>
<li><p>Sets the LOADDEV statement for a given virtual server. This command is only available in the development build of xCAT at this time.<br />The syntax is: <code>chvm &amp;lt;node&amp;gt; --setloaddev [wwpn] [lun]</code></p>
<pre><code># chvm gpok3 --setloaddev 5005076306138411 4014403000000000
gpok3: Setting LOADDEV directory statements
gpok3: Locking LINUX3... Done
gpok3: Replacing user entry of LINUX3... Done</code></pre></li>
<li><p>Sets the password for a given virtual server.<br />The syntax is: <code>chvm &amp;lt;node&amp;gt; --setpassword [password]</code></p>
<pre><code># chvm gpok3 --setpassword PSSWD
gpok3: Setting password for LNX3... Done</code></pre></li>
<li><p>Delete a dedicated device from a virtual machine's active configuration and directory entry.<br />The syntax is: <code>chvm &amp;lt;node&amp;gt; --undedicatedevice [virtual device]</code></p>
<pre><code># chvm gpok3 --undedicatedevice 1B89
gpok3: Deleting dedicated device from LNX3&#39;s directory entry... Done</code></pre></li>
</ul>
<p><code>rscan</code> - Collects the node information from one or more hardware control points.<br />The syntax is <code>rscan &amp;lt;zhcp&amp;gt; [-w]</code>. The <code>-w</code> option will populate the database with details collected by rscan.</p>
<pre><code># rscan gpok2
gpok2:
  objtype=node
  arch=s390x
  os=sles10sp3
  hcp=gpok3.endicott.ibm.com
  userid=LINUX2
  nodetype=vm
  parent=POKDEV61
  groups=all
  mgt=zvm</code></pre>
<p><code>rinv</code> - Remote hardware and software inventory. Options supported are:</p>
<ul>
<li><p>Collect the hardware and software inventory of a virtual machine.<br />The syntax is: <code>rinv &amp;lt;node&amp;gt; &amp;lt;all|config&amp;gt;</code>.</p>
<pre><code># rinv gpok3 all
gpok3: z/VM UserID: XCAT3
gpok3: z/VM Host: POKDEV61
gpok3:Operating System: SUSE Linux Enterprise Server 11 (s390x)
gpok3: Architecture:    s390x
gpok3: HCP: gpok3.endicott.ibm.com
gpok3: Privileges: 
gpok3:     Currently: G
gpok3:     Directory: G
gpok3: 
gpok3: Total Memory:    796M
gpok3: Processors: 
gpok3:     CPU 01  ID  FF0C452E20978000 CP   CPUAFF ON
gpok3:     CPU 00  ID  FF0C452E20978000 (BASE) CP   CPUAFF ON
gpok3: 
gpok3: Disks: 
gpok3:     DASD 0100 3390 EMC2C6 R/W      10016 CYL ON DASD  C2C6 SUBCHANNEL = 0000
gpok3:     DASD 0190 3390 EV61A2 R/O        107 CYL ON DASD  61A2 SUBCHANNEL = 000E
gpok3:     DASD 0191 3390 EMC20D R/O       1000 CYL ON DASD  C20D SUBCHANNEL = 0013
gpok3:     DASD 019D 3390 EV61A2 R/O        146 CYL ON DASD  61A2 SUBCHANNEL = 000F
gpok3:     DASD 019E 3390 EV61A2 R/O        250 CYL ON DASD  61A2 SUBCHANNEL = 0010
gpok3:     DASD 0300 9336 (VDSK) R/W     262144 BLK ON DASD  VDSK SUBCHANNEL = 0014
gpok3:     DASD 0301 9336 (VDSK) R/W     524288 BLK ON DASD  VDSK SUBCHANNEL = 0015
gpok3:     DASD 0402 3390 EV61A2 R/O        146 CYL ON DASD  61A2 SUBCHANNEL = 0011
gpok3:     DASD 0592 3390 EV61A2 R/O         70 CYL ON DASD  61A2 SUBCHANNEL = 0012
gpok3: 
gpok3: NICs:    
gpok3:     Adapter 0600.P00 Type: QDIO      Name: UNASSIGNED  Devices: 3
gpok3:       MAC: 02-00-06-00-05-38         LAN: * None
gpok3:     Adapter 0700.P00 Type: QDIO      Name: UNASSIGNED  Devices: 3
gpok3:       MAC: 02-00-06-00-05-39         LAN: * None
gpok3:     Adapter 0800.P00 Type: QDIO      Name: FOOBAR      Devices: 3
gpok3:       MAC: 02-00-06-00-05-3A         VSWITCH: SYSTEM VSW2</code></pre></li>
</ul>
<p>Note the complete inventory can only be retrieved when the node is online.</p>
<ul>
<li><p>Collect the hardware and software inventory of a z/VM system.<br />The syntax is: <code>rinv &amp;lt;node&amp;gt; &amp;lt;all|config&amp;gt;</code>.</p>
<pre><code># rinv pokdev61 all
pokdev61: z/VM Host: POKDEV61
pokdev61: zHCP: gpok3.endicott.ibm.com
pokdev61: Architecture: s390x
pokdev61: CEC Vendor: IBM
pokdev61: CEC Model: 2097
pokdev61: Hypervisor OS: z/VM 6.1.0
pokdev61: Hypervisor Name: POKDEV61
pokdev61: LPAR CPU Total: 10
pokdev61: LPAR CPU Used: 10
pokdev61: LPAR Memory Total: 16G
pokdev61: LPAR Memory Used: 0M
pokdev61: LPAR Memory Offline: 0</code></pre></li>
<li><p>List the configuration for a given disk pool.<br />The syntax is: <code>rinv &amp;lt;node&amp;gt; --diskpool [pool name] [space (free or used)]</code></p>
<pre><code># rinv pokdev61 --diskpool POOL1 free
pokdev61: #VolID DevType StartAddr Size
pokdev61: EMC2C4 3390-09 0001 10016
pokdev61: EMC2C5 3390-09 0001 10016</code></pre></li>
<li><p>List the disk pool names available.<br />The syntax is: <code>rinv &amp;lt;node&amp;gt; --diskpoolnames</code></p>
<pre><code># rinv pokdev61 --diskpoolnames
pokdev61: POOL1
pokdev61: POOL2
pokdev61: POOL3</code></pre></li>
<li><p>List the state of real FCP adapter devices on the z/VM system.<br />The syntax is: <code>rinv &amp;lt;node&amp;gt; --fcpdevices [state]</code>. The state can be either: active, free, or offline.</p>
<pre><code># rinv pokdev61 --fcpdevices free
pokdev61: B150
pokdev61: B151
pokdev61: B152
pokdev61: B153
pokdev61: B154
pokdev61: B155
pokdev61: B156
pokdev61: B157</code></pre></li>
<li><p>List the defined network names available on the z/VM system.<br />The syntax is: <code>rinv &amp;lt;node&amp;gt; --networknames</code></p>
<pre><code># rinv pokdev61 --networknames
pokdev61: LAN:QDIO SYSTEM GLAN1
pokdev61: LAN:HIPERS SYSTEM GLAN2
pokdev61: LAN:QDIO SYSTEM GLAN3
pokdev61: VSWITCH SYSTEM VLANTST1
pokdev61: VSWITCH SYSTEM VLANTST2
pokdev61: VSWITCH SYSTEM VSW1
pokdev61: VSWITCH SYSTEM VSW2
pokdev61: VSWITCH SYSTEM VSW3</code></pre></li>
<li><p>List the configuration for a given network.<br />The syntax is: <code>rinv &amp;lt;node&amp;gt; --network [networkname]</code></p>
<pre><code># rinv pokdev61 --network GLAN1
pokdev61: LAN SYSTEM GLAN1        Type: QDIO    Connected: 1    Maxconn: INFINITE
pokdev61:   PERSISTENT  UNRESTRICTED  IP                        Accounting: OFF
pokdev61:   IPTimeout: 5                 MAC Protection: Unspecified
pokdev61:   Isolation Status: OFF</code></pre></li>
<li><p>Obtain the SSI and system status.<br />The syntax is: <code>rinv &amp;lt;node&amp;gt; --ssi</code></p>
<pre><code># rinv pokdev62 --ssi
pokdev62: ssi_name = POKSSI
pokdev62: ssi_mode = Stable
pokdev62: ssi_pdr = CVD964_on_D964
pokdev62: cross_system_timeouts = Enabled
pokdev62: output.ssiInfoCount = 4
pokdev62: 
pokdev62: member_slot = 1
pokdev62: member_system_id = POKDEV62
pokdev62: member_state = Joined
pokdev62: member_pdr_heartbeat = 02/05/2013_22:34:40
pokdev62: member_received_heartbeat = 02/05/2013_22:34:40
pokdev62: 
pokdev62: member_slot = 2
pokdev62: member_system_id = POKTST62
pokdev62: member_state = Joined
pokdev62: member_pdr_heartbeat = 02/05/2013_22:34:28
pokdev62: member_received_heartbeat = 02/05/2013_22:34:28</code></pre></li>
<li><p>Obtain the SMAPI level installed on the z/VM system.</p>
<pre><code># rinv pokdev62 --smapilevel
pokdev62: The API functional level is z/VM V6.2</code></pre></li>
<li><p>Query all FCPs on a z/VM system and return a list of WWPNs.</p>
<pre><code># rinv pokdev62 --wwpn
pokdev62: 0000000000234567
pokdev62: 0000000000345678</code></pre></li>
<li><p>List the devices in a given zFCP device pool.<br />The syntax is: <code>rinv &amp;lt;node&amp;gt; --zfcppool [pool name] [space (free or used)]</code></p>
<pre><code># rinv pokdev61 --zfcppool zfcp1
pokdev61: #status,wwpn,lun,size,owner,channel,tag
pokdev61: free,500512345678c411,4012345100000000,2g,,,
pokdev61: used,500512345678c411,4012345200000000,8192M,gpok4,b15a,replace_root_device
pokdev61: free,500512345678c411,4012345300000000,8g,,,
pokdev61: free,500512345678c411,4012345400000000,2g,,,
pokdev61: free,500512345678c411,4012345600000000,2g,,,</code></pre></li>
<li><p>List the known zFCP pool names.</p>
<pre><code># rinv pokdev62 --zfcppoolnames
pokdev62: zfcp1
pokdev62: zfcp2</code></pre></li>
</ul>
<p><code>rmigrate</code> - Migrate VM from one z/VM member to another in an SSI cluster (only in z/VM 6.2). This command is only available in the development build of xCAT at this time.<br />The syntax is: <code>rmigrate &amp;lt;node&amp;gt; destination=[zvm member] action=[MOVE, TEST, or CANCEL] force=[ARCHITECTURE, DOMAIN, or STORAGE] immediate=[YES or NO] max_total=[total in seconds (optional)] max_quiesce=[quiesce in seconds(optional)]</code>. The key value pairs can be specified in any order.</p>
<ul>
<li><code>destination</code> is the name of the destination z/VM system to which the specified virtual machine will be relocated.</li>
<li><code>action</code> can be: (MOVE) initiate a VMRELOCATE MOVE of the VM, (TEST) determine if VM is eligible to be relocated, or (CANCEL) stop the relocation of VM.</li>
<li><code>force</code> can be: (ARCHITECTURE) attempt relocation even though hardware architecture facilities or CP features are not available on destination system, (DOMAIN) attempt relocation even though VM would be moved outside of its domain, or (STORAGE) relocation should proceed even if CP determines that there are insufficient storage resources on destination system.</li>
<li><code>immediate</code> can be: (YES) VMRELOCATE command will do one early pass through virtual machine storage and then go directly to the quiesce stage, or (NO) specifies immediate processing.</li>
<li><code>max_total</code> is the maximum wait time for relocation to complete. Optional.</li>
<li><p><code>max_quiesce</code> is the maximum quiesce time a VM may be stopped during a relocation attempt. Optional.</p>
<h1><a href="#TOC">rmigrate gpok3 destination=poktst62 action=MOVE immediate=NO force=&quot;ARCHITECTURE DOMAIN STORAGE&quot;</a></h1>
<p>gpok3: Running VMRELOCATE action=MOVE against LNX3... Done</p></li>
</ul>
<p><code>xdsh</code> - Concurrently runs commands on multiple nodes.<br />The syntax is: <code>xdsh &amp;lt;node&amp;gt; -e &amp;lt;script&amp;gt;</code>.</p>
<pre><code># xdsh gpok3 /tmp/myScript.sh</code></pre>
<p>For a list of general xCAT commands, <a href="http://xcat.sourceforge.net/man1/xcat.1.html">click here</a>.</p>
<p>For some commands above, such as <code>chvm</code>, a return code and reason code may be returned. xCAT will translate the meaning for most of these codes. In such cases where it could not, refer to the <a href="http://publib.boulder.ibm.com/infocenter/zvm/v6r2/topic/com.ibm.zvm.v620.dmse6/hcsl8c1167.htm?path=6_18_5_2_0#wq1561">Systems Management Application Programming (SMAPI)</a> documentation, which lists the return codes and their description.</p>
<p>In some cases, a return code of 596 may be returned. In this case, take the reason code that follows it and decipher it using the <a href="http://publib.boulder.ibm.com/infocenter/zvm/v5r4/topic/com.ibm.zvm.v54.hcpk2/msgs.htm#msgs">Directory Maintenance Facility Messages</a> documentation.</p>
<h2 id="installing-linux-using-autoyast-or-kickstart"><a href="#TOC">Installing Linux Using AutoYast or Kickstart</a></h2>
<p>This section provides details on the installation of Linux using autoyast or kickstart.</p>
<p>There are two ways to install Linux onto a z/VM virtual server, depending on which Linux distribution you want. One is through autoyast, which is used to install SUSE Linux Enterprise Server (SLES) releases. The other is through kickstart, which is used to install Red Hat Enterprise Linux (RHEL) releases.</p>
<p>Before you begin, make sure the following is done.</p>
<ul>
<li>The FTP server must be setup during the xCAT MN installation, and the FTP root directory (/install) must contain the appropriate Linux distribution.</li>
<li>If you are managing an IP address range starting at 1 (e.g. 10.1.100.1), be sure that the netmask is set correctly (e.g. 255.255.255.0) on the xCAT MN or else the node you are trying to provision cannot find the repository.</li>
</ul>
<p>In the following example, we will provision a new node (gpok3) with a userID (LNX3) that is managed by our zHCP (gpok2). You will need to substitute the node name, userID, and zHCP name with appropriate values.</p>
<ol style="list-style-type: decimal">
<li>Logon the xCAT MN as root using a Putty terminal</li>
<li><p>Create the node definition</p>
<pre><code># mkdef -t node -o gpok3 userid=LNX3 hcp=gpok2.endicott.ibm.com mgt=zvm groups=all
Object definitions have been created or modified.</code></pre></li>
</ol>
<p>Set the node's IP address and hostname (only if a regex is not set for the group)</p>
<pre><code>    # chtab node=gpok3 hosts.ip=&quot;10.1.100.3&quot; hosts.hostnames=&quot;gpok3.endicott.ibm.com&quot;</code></pre>
<ol start="3" style="list-style-type: decimal">
<li><p>Update /etc/hosts</p>
<pre><code># makehosts</code></pre></li>
<li><p>Update DNS</p>
<pre><code># makedns</code></pre></li>
<li><p>Define the directory entry for the new virtual server in a text file (dirEntry.txt). For our example, we used the following:</p>
<pre><code>USER LNX3 PWD 512M 1G G
INCLUDE LNXDFLT
COMMAND SET VSWITCH VSW2 GRANT LNX3
COMMAND COUPLE 0800 SYSTEM VSW2</code></pre></li>
</ol>
<p>Once you have defined the directory entry in a text file, create the virtual server by issuing the following command (the full file path must be given):</p>
<pre><code>    # mkvm gpok3 /tmp/dirEntry.txt
    gpok3: Creating user directory entry for LNUX3... Done</code></pre>
<p>The directory entry text file should not contain any extra new lines (/n). A MAC address will be assigned to the user ID upon creation.</p>
<ol start="6" style="list-style-type: decimal">
<li>Copy the default autoyast/kickstart template and package list available in xCAT (if not already). Customize this template and package list (the ones you copied) to how you see fit. For more information on how to customize the template, see Appendix B.</li>
</ol>
<p>If you want to install a SUSE Linux Enterprise Server:</p>
<pre><code>    # mkdir -p /install/custom/install/sles
    # cp /opt/xcat/share/xcat/install/sles/compute.sles10.s390x.tmpl /install/custom/install/sles
    # cp /opt/xcat/share/xcat/install/sles/compute.sles10.s390x.pkglist /install/custom/install/sles
    # cp /opt/xcat/share/xcat/install/sles/compute.sles11.s390x.tmpl /install/custom/install/sles
    # cp /opt/xcat/share/xcat/install/sles/compute.sles11.s390x.pkglist /install/custom/install/sles</code></pre>
<p>There are two templates available for SLES, one for SLES 10 (compute.sles10.s390x.tmpl) and the other for SLES 11 (compute.sles11.s390x.tmpl). It is recommended that you copy both templates into /install/custom/install/sles.</p>
<p>If you want to install a Red Hat Enterprise Linux:</p>
<pre><code>    # mkdir -p /install/custom/install/rh
    # cp /opt/xcat/share/xcat/install/rh/compute.rhel5.s390x.tmpl /install/custom/install/rh
    # cp /opt/xcat/share/xcat/install/rh/compute.rhel5.s390x.pkglist /install/custom/install/rh
    # cp /opt/xcat/share/xcat/install/rh/compute.rhels6.s390x.tmpl /install/custom/install/rh/compute.rhel6.s390x.tmpl
    # cp /opt/xcat/share/xcat/install/rh/compute.rhels6.s390x.pkglist /install/custom/install/rh/compute.rhel6.s390x.pkglist</code></pre>
<p>There are also two templates available for RHEL, one for RHEL 5 (compute.rhel5.s390x.tmpl) and the other for RHEL 6 (compute.rhels6.s390x.tmpl). It is recommended that you copy both templates into /install/custom/install/rh.</p>
<p>The default templates are configured to use one 3390-mod9 with / mounted and use DHCP. The package lists (.pkglist) are configured to install the base software package. You should only customize the disks, partitioning, and install packages, and leave the network configuration alone. xCAT will handle the network configuration based on the xCAT hosts and networks table.</p>
<ol start="7" style="list-style-type: decimal">
<li><p>Add disks to the new node (the default autoyast/kickstart template available in xCAT requires 1 3390-MOD9 disks attached at 0100).</p>
<pre><code># chvm gpok3 --add3390 POOL1 0100 10016 MR
gpok3: Adding disk 0100 to LNX3... Done</code></pre></li>
</ol>
<p>Be sure that each disk in the pool is attached to SYSTEM.</p>
<p>Alternatively, you can use SCSI/FCP disks (which are seen by z/VM as 9336 disks), but you first need to configure the autoyast/kickstart template. See Appendix B for details. If you choose to have SCSI/FCP disks, you can add these disks to the new node using:</p>
<pre><code>    # chvm gpok3 --add9336 POOL3 0101 512 4194272 MR
    gpok3: Adding disk 0100 to LNX3... Done</code></pre>
<ol start="8" style="list-style-type: decimal">
<li><p>Set up the noderes and nodetype tables. You need to determine the OS and profile (autoyast/kickstart template) for the node. Here, we have nodetype.os=sles10sp3. You can find available OS and profiles by issuing:</p>
<pre><code># tabdump osimage</code></pre></li>
</ol>
<p>If you want to install a SUSE Linux Enterprise Server:</p>
<pre><code>    # chtab node=gpok3 noderes.netboot=zvm nodetype.os=sles10sp3 nodetype.arch=s390x nodetype.profile=compute</code></pre>
<p>If you want to install a Red Hat Enterprise Linux:</p>
<pre><code>    # chtab node=gpok3 noderes.netboot=zvm nodetype.os=rhel5.4 nodetype.arch=s390x nodetype.profile=compute</code></pre>
<ol start="9" style="list-style-type: decimal">
<li><p>(Optional) If the xCAT MN is on multiple networks, such as 10.1.100.0/24 and 192.168.100.0/24, then you will need to specify the address to use for it during provisioning. For example, if the node you are provisioning is on the 192.168.100.0/24 network, and the IP address of the xCAT MN specified in site.master is 10.1.100.1, then you need to specify issue:</p>
<pre><code># nodech gpok3 noderes.xcatmaster=192.168.100.1</code></pre></li>
</ol>
<p>This is assuming 192.168.100.1 is the IP address of the xCAT MN on the 192.168.100.0/24 network. You only need to run the command if the node is on a different network than the one specified in site.master. This allows the autoyast/kickstart template and the software repository to be found for installation.</p>
<ol start="10" style="list-style-type: decimal">
<li><p>Verify the definition</p>
<pre><code># lsdef gpok3</code></pre></li>
</ol>
<p>It should look similar to this:</p>
<pre><code>    Object name: gpok3
         arch=s390x
         groups=all
         hcp=gpok2.endicott.ibm.com
         hostnames=gpok3.endicott.ibm.com
         ip=10.1.100.3
         mac=02:00:01:FF:FF:F0
         mgt=zvm
         netboot=zvm
         os=sles10sp3
         postbootscripts=otherpkgs
         postscripts=syslog,remoteshell,syncfiles
         profile=compute
         userid=LNX3</code></pre>
<ol start="11" style="list-style-type: decimal">
<li><p>Add the new node to DHCP</p>
<pre><code># makedhcp -a</code></pre></li>
<li><p>Prepare the new node for installation</p>
<pre><code># nodeset gpok3 install
gpok3: Purging reader... Done
gpok3: Punching kernel to reader... Done
gpok3: Punching parm to reader... Done
gpok3: Punching initrd to reader... Done
gpok3: Kernel, parm, and initrd punched to reader.  Ready for boot.</code></pre></li>
<li><p>Boot the new node from reader</p>
<pre><code># rnetboot gpok3 ipl=00C
gpok3: Starting LNX3... Done

gpok3: Booting from 00C... Done</code></pre></li>
<li><p>In Gnome or KDE, open the VNC viewer to see the installation progress. It might take a couple of minutes before you can connect.</p>
<pre><code># vncviewer gpok3:1</code></pre></li>
</ol>
<p>The default VNC password is 12345678. If you have trouble connecting to the vncviewer, open a 3270 console to the node, try steps 11 and 12 again, and look at the progress on the console.</p>
<ol start="15" style="list-style-type: decimal">
<li><p>(Only for SLES 10 SP2 or older) Once the first phase of installation is complete, restart the virtual server to complete the final phase of installation</p>
<pre><code># rpower gpok3 reset</code></pre></li>
<li><p>The default password for the node can be found in the passwd table. See <a href="https://sourceforge.net/apps/mediawiki/xcat/index.php?title=XCAT_zVM_Setup#Initializing_Database">Initializing Database</a> section step 4. The SSH keys should already be setup for the node.</p></li>
</ol>
<h2 id="installing-linux-using-scsifcp"><a href="#TOC">Installing Linux Using SCSI/FCP</a></h2>
<p>This section provides details on the installation of Linux using SCSI/FCP. This feature is only available in the development build of xCAT at this time. xCAT has limited support for SCSI/FCP devices. Features such as NPIV are not currently supported, but will be in subsequent releases.</p>
<ol style="list-style-type: decimal">
<li>Logon the xCAT MN as root using a Putty terminal</li>
<li><p>Create the z/VM hypervisor definition (if not already)</p>
<pre><code># nodeadd pokdev61 groups=hosts hypervisor.type=zvm nodetype.os=zvm6.1 zvm.hcp=gpok2.endicott.ibm.com mgt=zvm</code></pre></li>
<li><p>Create the node definition</p>
<pre><code># mkdef -t node -o gpok4 userid=LNX4 hcp=gpok2.endicott.ibm.com mgt=zvm groups=all
Object definitions have been created or modified.</code></pre></li>
</ol>
<p>Set the node's IP address and hostname (only if a regex is not set for the group)</p>
<pre><code>    # chtab node=gpok4 hosts.ip=&quot;10.1.100.4&quot; hosts.hostnames=&quot;gpok4.endicott.ibm.com&quot;</code></pre>
<ol start="4" style="list-style-type: decimal">
<li><p>Update /etc/hosts</p>
<pre><code># makehosts</code></pre></li>
<li><p>Update DNS</p>
<pre><code># makedns</code></pre></li>
<li><p>Define the directory entry for the new virtual server in a text file (dirEntry.txt). For our example, we used the following:</p>
<pre><code>USER LNX4 PWD 512M 1G G
INCLUDE LNXDFLT
COMMAND SET VSWITCH VSW2 GRANT LNX4</code></pre></li>
</ol>
<p>Once you have defined the directory entry in a text file, create the virtual server by issuing the following command (the full file path must be given):</p>
<pre><code>    # mkvm gpok4 /tmp/dirEntry.txt
    gpok4: Creating user directory entry for LNX4... Done</code></pre>
<p>The directory entry text file should not contain any extra new lines (/n). A MAC address will be assigned to the user ID upon creation.</p>
<ol start="7" style="list-style-type: decimal">
<li>Copy the default autoyast/kickstart template and package list available in xCAT (if not already). Customize this template and package list (the ones you copied) to how you see fit. For more information on how to customize the template, see Appendix B.</li>
</ol>
<p>If you want to install a SUSE Linux Enterprise Server:</p>
<pre><code>    # mkdir -p /install/custom/install/sles
    # cp /opt/xcat/share/xcat/install/sles/zfcp.sles10.s390x.tmpl /install/custom/install/sles
    # cp /opt/xcat/share/xcat/install/sles/zfcp.sles10.s390x.pkglist /install/custom/install/sles
    # cp /opt/xcat/share/xcat/install/sles/zfcp.sles11.s390x.tmpl /install/custom/install/sles
    # cp /opt/xcat/share/xcat/install/sles/zfcp.sles11.s390x.pkglist /install/custom/install/sles</code></pre>
<p>There are two templates available for SLES, one for SLES 10 (zfcp.sles10.s390x.tmpl) and the other for SLES 11 (zfcp.sles11.s390x.tmpl). It is recommended that you copy both templates into /install/custom/install/sles.</p>
<p>If you want to install a Red Hat Enterprise Linux:</p>
<pre><code>    # mkdir -p /install/custom/install/rh
    # cp /opt/xcat/share/xcat/install/rh/zfcp.rhel5.s390x.tmpl /install/custom/install/rh
    # cp /opt/xcat/share/xcat/install/rh/zfcp.rhel5.s390x.pkglist /install/custom/install/rh
    # cp /opt/xcat/share/xcat/install/rh/zfcp.rhels6.s390x.tmpl /install/custom/install/rh/zfcp.rhel6.s390x.tmpl
    # cp /opt/xcat/share/xcat/install/rh/zfcp.rhels6.s390x.pkglist /install/custom/install/rh/zfcp.rhel6.s390x.pkglist</code></pre>
<p>There are also two templates available for RHEL, one for RHEL 5 (zfcp.rhel5.s390x.tmpl) and the other for RHEL 6 (zfcp.rhels6.s390x.tmpl). It is recommended that you copy both templates into /install/custom/install/rh.</p>
<p>The default templates are configured to use one SCSI device with / mounted and use DHCP. The package lists (.pkglist) are configured to install the base software package. You should only customize the disks, partitioning, and install packages, and leave the network configuration alone. xCAT will handle the network configuration based on the xCAT hosts and networks table.</p>
<ol start="8" style="list-style-type: decimal">
<li><p>Find a suitable FCP channel to dedicate to the virtual server.</p>
<pre><code># rinv pokdev61 --fcpdevices free
pokdev61: B150
pokdev61: B151
pokdev61: B152
pokdev61: B153
pokdev61: B154
pokdev61: B155
pokdev61: B156
pokdev61: B157
pokdev61: B158
pokdev61: B159
pokdev61: B15A</code></pre></li>
<li><p>Dedicate the FCP channel to the virtual server</p>
<pre><code># chvm gpok4 --dedicatedevice B15A B15A 0
gpok4: Dedicating device B15A as B15A to LNX4... Done</code></pre></li>
<li><p>Add disks to the new node (the default autoyast/kickstart template available in xCAT requires 1 SCSI/FCP disk with enough space for a Linux operating system). Note that a tag (<code>replace_root_device</code>) is specified. This tag identifies how the device will be partitioned in the autoyast/kickstart template. The appropriate WWPN/LUN will be substituted in place of this tag later on with the <code>nodeset</code> command. Also, one disk must be set as the LOADDEV device, allowing the virtual server to boot from it at IPL.</p>
<pre><code># chvm gpok4 --addzfcp zfcp1 b15a 1 8g replace_root_device
gpok4: Using device with WWPN/LUN of 500501234567C890/4012345600000000
gpok4: Adding FCP device... Done
gpok4: Setting LOADDEV directory statements
gpok4: Locking LNX4... Done
gpok4: Replacing user entry of LNX4... Done</code></pre></li>
<li><p>Set up the noderes and nodetype tables. You need to determine the OS and profile (autoyast/kickstart template) for the node. Here, we have nodetype.os=sles11sp2. You can find available OS and profiles by issuing:</p>
<pre><code># tabdump osimage</code></pre></li>
</ol>
<p>If you want to install a SUSE Linux Enterprise Server:</p>
<pre><code>    # chtab node=gpok4 noderes.netboot=zvm nodetype.os=sles11sp2 nodetype.arch=s390x nodetype.profile=zfcp</code></pre>
<p>If you want to install a Red Hat Enterprise Linux:</p>
<pre><code>     # chtab node=gpok4 noderes.netboot=zvm nodetype.os=rhel6.2 nodetype.arch=s390x nodetype.profile=zfcp</code></pre>
<ol start="12" style="list-style-type: decimal">
<li><p>(Optional) If the xCAT MN is on multiple networks, such as 10.1.100.0/24 and 192.168.100.0/24, then you will need to specify the address to use for it during provisioning. For example, if the node you are provisioning is on the 192.168.100.0/24 network, and the IP address of the xCAT MN specified in site.master is 10.1.100.1, then you need to specify issue:</p>
<pre><code># nodech gpok4 noderes.xcatmaster=192.168.100.1</code></pre></li>
</ol>
<p>This is assuming 192.168.100.1 is the IP address of the xCAT MN on the 192.168.100.0/24 network. You only need to run the command if the node is on a different network than the one specified in site.master. This allows the autoyast/kickstart template and the software repository to be found for installation.</p>
<ol start="13" style="list-style-type: decimal">
<li><p>Verify the definition</p>
<pre><code># lsdef gpok4</code></pre></li>
</ol>
<p>It should look similar to this:</p>
<pre><code>    Object name: gpok4
         arch=s390x
         groups=all
         hcp=gpok2.endicott.ibm.com
         hostnames=gpok4.endicott.ibm.com
         ip=10.1.100.4
         mac=02:00:01:FF:FF:EF
         mgt=zvm
         netboot=zvm
         os=sles11sp2
         postbootscripts=otherpkgs
         postscripts=syslog,remoteshell,syncfiles
         profile=compute
         userid=LNX4</code></pre>
<ol start="14" style="list-style-type: decimal">
<li><p>Add the new node to DHCP</p>
<pre><code># makedhcp -a</code></pre></li>
<li><p>Prepare the new node for installation</p>
<pre><code># nodeset gpok4 install
gpok4: Inserting FCP devices into template... Done
gpok4: Purging reader... Done
gpok4: Punching kernel to reader... Done
gpok4: Punching parm to reader... Done
gpok4: Punching initrd to reader... Done
gpok4: Kernel, parm, and initrd punched to reader.  Ready for boot.</code></pre></li>
<li><p>Boot the new node from reader</p>
<pre><code># rnetboot gpok4 ipl=00C
gpok3: Starting LNX4... Done

gpok3: Booting from 00C... Done</code></pre></li>
<li><p>In Gnome or KDE, open the VNC viewer to see the installation progress. It might take a couple of minutes before you can connect.</p>
<pre><code># vncviewer gpok4:1</code></pre></li>
</ol>
<p>The default VNC password is 12345678. If you have trouble connecting to the vncviewer, open a 3270 console to the node, try steps 11 and 12 again, and look at the progress on the console.</p>
<ol start="18" style="list-style-type: decimal">
<li><p>(Only for SLES 10 SP2 or older) Once the first phase of installation is complete, restart the virtual server to complete the final phase of installation</p>
<pre><code># rpower gpok4 reset</code></pre></li>
<li><p>The default password for the node can be found in the passwd table. See <a href="https://sourceforge.net/apps/mediawiki/xcat/index.php?title=XCAT_zVM_Setup#Initializing_Database">Initializing Database</a> section step 4. The SSH keys should already be setup for the node.</p></li>
</ol>
<h2 id="adding-software-packages"><a href="#TOC">Adding Software Packages</a></h2>
<p>This section shows how to add other software packages (ones available outside the OS distribution) into the autoyast/kickstart installation process.</p>
<p>In the following example, we will add Ganglia (packaged with xCAT) and configure it during the autoyast/kickstart installation.</p>
<ol style="list-style-type: decimal">
<li><p>Put the RPMs you want to be installed under <code>/install/post/otherpkgs/&amp;lt;os&amp;gt;/&amp;lt;arch&amp;gt;</code> directory on the xCAT MN, where <code>&amp;lt;os&amp;gt;</code> and <code>&amp;lt;arch&amp;gt;</code> can be found in the nodetype table. It is important to note that the <code>&amp;lt;os&amp;gt;</code> name must match what is in the nodetype table. If it does not match, the additional software packages will not be installed.</p>
<pre><code># mkdir -p /install/post/otherpkgs/sles11sp1/s390x 
# cp /root/xcat/xcat-dep/sles11/s390x/ganglia-gmond-3.1.1-1.s390x.rpm /install/post/otherpkgs/sles11sp1/s390x
# cp /root/xcat/xcat-dep/sles11/s390x/libganglia-3.1.1-1.s390x.rpm /install/post/otherpkgs/sles11sp1/s390x
# cp /root/xcat/xcat-dep/sles11/s390x/libconfuse-2.6-1.s390x.rpm /install/post/otherpkgs/sles11sp1/s390x</code></pre></li>
<li><p>Create a repository under the directory where you put the RPMs. Every time the RPM version is updated, you will need to recreate this repository using <code>createrepo</code>.</p>
<pre><code># cd /install/post/otherpkgs/sles11sp1/s390x
# createrepo .
Saving Primary metadata
Saving file lists metadata
Saving other metadata</code></pre></li>
<li><p>Put the package names (in our case, libconfuse, libganglia, and ganglia-gmond) to be installed in <code>/install/custom/install/&amp;lt;os&amp;gt;/&amp;lt;profile&amp;gt;.&amp;lt;os&amp;gt;.otherpkgs.pkglist</code>. For example</p>
<pre><code># cat /install/custom/install/sles/compute.sles11sp1.otherpkgs.pkglist
libconfuse 
libganglia 
ganglia-gmond</code></pre></li>
</ol>
<p>The autoyast/kickstart install process picks up the RPMs listed in the otherpkgs.pkglist and installs them on to the nodes.</p>
<ol start="4" style="list-style-type: decimal">
<li><p>Most software packages require some kind of configuration. In the case of Ganglia, gmond needs to be configured to advertise to gmetad (on the xCAT MN). The configuration can be done using postscripts.<br />Place the following script under /install/postscripts.</p>
<pre><code># cat /install/postscripts/confGanglia

#!/bin/sh
# Post-script to customize virtual machine

# Install Ganglia
echo &quot;Configuring Ganglia...&quot;

# Get IP address of MS
OS=`uname`
echo &quot;The OS is: $OS&quot;
ms_ip=$MONMASTER
result=`ping -c1 $MONMASTER 2&amp;gt;&amp;1`
if [ $? -eq 0 ]; then
    index1=`expr index &quot;$result&quot; &quot;\(&quot;`
    index2=`expr index &quot;$result&quot; &quot;\)&quot;`
    pos=`expr $index1 + 1`
    length=`expr $index2 - $index1`
    length=`expr $length - 1`
    ms_ip=`expr substr &quot;$result&quot; $pos $length`
    echo &quot;MS IP is: $ms_ip&quot;
fi

CLUSTER=\&quot;$MONSERVER\&quot;
echo &quot;Cluster is: $CLUSTER&quot;
MASTER=$ms_ip
gmond_conf=&quot;/etc/ganglia/gmond.conf&quot;
gmond_conf_old=&quot;/etc/gmond.conf&quot;
if [ $OS != &quot;AIX&quot; ]; then
    if [ -f  $gmond_conf ]; then
        grep &quot;xCAT gmond settings done&quot; $gmond_conf
        if [ $? -gt 0 ]; then
            /bin/cp -f $gmond_conf /etc/ganglia/gmond.conf.orig
            sed -i &#39;s/setuid = yes/setuid = no/1&#39; $gmond_conf
            sed -i &#39;s/name = &quot;unspecified&quot;/name=&#39;$CLUSTER&#39;/1&#39; $gmond_conf
            sed -e &quot;1,40s/mcast_join = .*/host = $MASTER/&quot; $gmond_conf &amp;gt; /etc/temp.conf
            /bin/cp -f /etc/temp.conf $gmond_conf
            sed -i &#39;s/mcast_join/#/g&#39; $gmond_conf
            sed -i &#39;s/bind/#/g&#39; $gmond_conf
            echo &quot;# xCAT gmond setup end&quot; &amp;gt;&amp;gt; $gmond_conf
        fi
    fi
fi

if [ $OS != &quot;AIX&quot; ]; then
    if [ -f $gmond_conf_old ]; then
        grep &quot;xCAT gmond settings done&quot; $gmond_conf_old
        if [ $? -gt 0 ]; then
            /bin/cp -f $gmond_conf_old /etc/gmond.conf.orig
            sed -i &#39;s/setuid = yes/setuid = no/1&#39; $gmond_conf_old
            sed -i &#39;s/name = &quot;unspecified&quot;/name=&#39;$CLUSTER&#39;/1&#39; $gmond_conf_old
            sed -e &quot;1,40s/mcast_join = .*/host = $MASTER/&quot; $gmond_conf_old &amp;gt; /etc/temp.conf
            /bin/cp -f /etc/temp.conf $gmond_conf_old
            sed -i &#39;s/mcast_join/#/g&#39; $gmond_conf_old
            sed -i &#39;s/bind/#/g&#39; $gmond_conf_old
            echo &quot;# xCAT gmond settings done sh_old&quot; &amp;gt;&amp;gt; $gmond_conf_old
        fi 
    fi
fi

# Start gmond
/etc/init.d/gmond start</code></pre></li>
<li><p>Give the appropriate file permissions for the script</p>
<pre><code># chmod 755 /install/postscripts/confGanglia</code></pre></li>
<li><p>Specify the postscript to run at install time by putting it in the postscripts table in xCAT (using <code>tabedit</code>). In the case of Ganglia, the <code>otherpkgs</code> and <code>confGanglia</code> scripts need to be run after installation. <code>otherpkgs</code> script comes packaged with xCAT and <code>confGanglia</code> script is provided above.</p>
<pre><code># tabdump postscripts

#node,postscripts,postbootscripts,comments,disable
&quot;xcatdefaults&quot;,&quot;syslog,remoteshell,syncfiles&quot;,&quot;otherpkgs&quot;,,
&quot;all&quot;,&quot;otherpkgs,confGanglia&quot;,,,</code></pre></li>
<li><p>You can optionally install other packages (e.g. Ganglia) after the autoyast/kickstart installation process by using: <code>updatenode &amp;lt;node&amp;gt; otherpkgs</code>. The node must be online for this to work.</p>
<pre><code># updatenode gpok3 otherpkgs
gpok3: Running postscript: otherpkgs
gpok3: NFSERVER=10.1.100.1
gpok3: OTHERPKGDIR=10.1.100.1/post/otherpkgs/sles11sp1/s390x
gpok3: Repository &#39;SUSE-Linux-Enterprise-Server-11-SP1 11.1.1-1.152&#39; is up to date.
gpok3: Repository &#39;sles11sp1&#39; is up to date.
gpok3: All repositories have been refreshed.
gpok3: zypper --non-interactive update --auto-agree-with-license
gpok3: Loading repository data...
gpok3: Reading installed packages...
gpok3: 
gpok3: Nothing to do.
gpok3: rpm -Uvh --replacepkgs  libconfuse* libganglia* ganglia-gmond*
gpok3: warning: libconfuse-2.6-1.s390x.rpm: Header V3 DSA signature: NOKEY, key ID da736c68
gpok3: Preparing...                ##################################################
gpok3: libconfuse                  ##################################################
gpok3: libganglia                  ##################################################
gpok3: ganglia-gmond               ##################################################
gpok3: insserv: warning: script &#39;S11xcatpostinit1&#39; missing LSB tags and overrides
gpok3: insserv: warning: script &#39;xcatpostinit1&#39; missing LSB tags and overrides
gpok3: gmond                     0:off  1:off  2:off  3:on   4:off  5:on   6:off
gpok3: Running of postscripts has completed.</code></pre></li>
</ol>
<h2 id="cloning-virtual-servers"><a href="#TOC">Cloning Virtual Servers</a></h2>
<p>This section shows how to clone a virtual server running Linux. Cloning is only supported on ECKD and FBA devices.</p>
<p>In the following example, we will clone the virtual server that we created (gpok3) in the previous section <em>Installing Linux Using Autoyast or Kickstart</em>. The new virtual server will have the node name (gpok4) and user ID (LNX4) respectively, and managed by the same zHCP (gpok2). You will need to substitute the node name, user ID, and zHCP name with appropriate values.</p>
<ol style="list-style-type: decimal">
<li>Logon the xCAT MN as root using a Putty terminal (if not already)</li>
<li><p>The source node must be online and accessible via SSH. If it is not online, bring it online.</p>
<pre><code># rpower gpok3 on</code></pre></li>
<li><p>Setup the SSH keys for the source node to be cloned (if not already)</p>
<pre><code># xdsh gpok3 -K</code></pre></li>
<li><p>Create the table definition for new node (gpok4)</p>
<pre><code># mkdef -t node -o gpok4 userid=LNX4 hcp=gpok2.endicott.ibm.com mgt=zvm groups=all</code></pre></li>
</ol>
<p>Set the node's IP address and hostname (only if a regex is not set for the group)</p>
<pre><code>    # chtab node=gpok4 hosts.ip=&quot;10.1.100.4&quot; hosts.hostnames=&quot;gpok4.endicott.ibm.com&quot;</code></pre>
<ol start="5" style="list-style-type: decimal">
<li><p>Update /etc/hosts</p>
<pre><code># makehosts</code></pre></li>
<li><p>Update DNS</p>
<pre><code># makedns</code></pre></li>
<li><p>Add the new node to DHCP</p>
<pre><code># makedhcp -a</code></pre></li>
<li><p>In order to clone a virtual server running Linux, the partition must be mounted by path. This is done by default for the node (gpok3) that we created in the previous section and in general, for nodes provision by xCAT using the default templates.</p></li>
</ol>
<p>For SUSE Linux Enterprise Server:<br />The root directory under /etc/fstab, which contains information on the system partitions and disks, should be similar to this:</p>
<pre><code>    /dev/disk/by-path/ccw-0.0.0100-part1  /  ext3  acl,user_xattr  1 1</code></pre>
<p>The parameters under /etc/zipl.conf, which specifies which disks to bring online when the system is IPLed, should be similar to this:</p>
<pre><code>    parameters = &quot;root=/dev/disk/by-path/ccw-0.0.0100-part1 TERM=dumb&quot;</code></pre>
<p>If you happen to edit zipl.conf, you must run <code>zipl</code> after you made the changes so that changes are written to the boot record.</p>
<ol start="9" style="list-style-type: decimal">
<li><p>Clone virtual server(s) running Linux:</p>
<pre><code># mkvm gpok4 gpok3 pool=POOL1
gpok4: Cloning gpok3
gpok4: Linking source disk (0100) as (1100)
gpok4: Stopping LNX3... Done

gpok4: Creating user directory entry
gpok4: Granting VSwitch (VSW2) access for LNX3
gpok4: Adding minidisk (0100)
gpok4: Disks added (1). Disks in user entry (1)
gpok4: Linking target disk (0100) as (2100)
gpok4: Copying source disk (1100) to target disk (2100) using FLASHCOPY
gpok4: Mounting /dev/dasde1 to /mnt/LNX3
gpok4: Setting network configuration
gpok4: Powering on
gpok4: Detatching source disk (0100) at (1100)
gpok4: Starting LNX3... Done

gpok4: Done</code></pre></li>
</ol>
<p>This will create a virtual server (gpok4) identical to gpok3. It will use disks in disk pool POOL1.</p>
<p>If FLASHCOPY is not enabled on your z/VM system, then this will take several minutes to complete depending on the number of nodes you want to clone. Also, FLASHCOPY will not work if the disks are not on the same storage facility.</p>
<ol start="10" style="list-style-type: decimal">
<li><p>Check the boot status of the node by pinging it:</p>
<pre><code># pping gpok4
gpok4: ping</code></pre></li>
</ol>
<p>If the node returns a ping, then it is fully booted and you can start using it. If you try to SSH into the node and are prompted for a password, you need to setup the SSH keys for each for the new nodes:</p>
<pre><code>    # xdsh gpok4 -K
    Enter the password for the userid: root on the node where the ssh keys
    will be updated:

    /usr/bin/ssh setup is complete.
    return code = 0</code></pre>
<h2 id="setting-up-ganglia-on-xcat"><a href="#TOC">Setting Up Ganglia on xCAT</a></h2>
<p>This section details how to the set up Ganglia on Linux on System z.</p>
<h3 id="red-hat-enterprise-linux"><a href="#TOC">Red Hat Enterprise Linux</a></h3>
<p>If you have Red Hat Enterprise Linux, follow the instructions below.</p>
<ol style="list-style-type: decimal">
<li>Logon the xCAT MN as root using a Putty terminal (if not already)</li>
<li><p>Go into the directory where you extracted the xcat-dep tarball, e.g. /root/xcat. Locate the Ganglia RPMs under /root/xcat/xcat-dep/&lt;os&gt;/s390x, where &lt;os&gt; is the RHEL version you are running. Verify install the following RPMs are present.</p>
<pre><code>rrdtool-1.4.5-0.20.s390x.rpm (RHEL 5.x only)
libconfuse-2.6-1.s390x.rpm
libganglia-3.1.1-1.s390x.rpm
ganglia-gmetad-3.1.1-1.s390x.rpm
ganglia-gmond-3.1.1-1.s390x.rpm
ganglia-web-3.1.1-1.s390x.rpm</code></pre></li>
<li>Set up ganglia on the xCAT MN
<ul>
<li><p>Install PHP and apache packages (if not already). Use yast to install the following packages</p>
<pre><code># yum install apr pkgconfig php-pear php-gd httpd</code></pre></li>
<li><p>Install the Ganglia RPMs</p>
<pre><code># yum install ganglia-gmetad ganglia-gmond ganglia-web
Loaded plugins: product-id, subscription-manager
Updating Red Hat repositories.
Setting up Install Process
Resolving Dependencies
--&amp;gt; Running transaction check
---&amp;gt; Package ganglia-gmetad.s390x 0:3.1.1-1 will be installed
---&amp;gt; Package ganglia-gmond.s390x 0:3.1.1-1 will be installed
---&amp;gt; Package ganglia-web.s390x 0:3.1.1-1 will be installed
--&amp;gt; Finished Dependency Resolution

Dependencies Resolved



###### ====================================================================


 Package                Arch          Version            Repository        Size


###### ====================================================================


Installing:
 ganglia-gmetad         s390x         3.1.1-1            xcat-dep          39 k
 ganglia-gmond          s390x         3.1.1-1            xcat-dep         283 k
 ganglia-web            s390x         3.1.1-1            xcat-dep         112 k

Transaction Summary


###### ====================================================================


Install       3 Package(s)

Total download size: 435 k
Installed size: 1.2 M
Is this ok [y/N]: y
Downloading Packages:


* * *


Total                                            79 MB/s | 435 kB     00:00     
Running rpm_check_debug
Running Transaction Test
Transaction Test Succeeded
Running Transaction
  Installing : ganglia-gmetad-3.1.1-1.s390x                                 1/3 
  Installing : ganglia-web-3.1.1-1.s390x                                    2/3 
  Installing : ganglia-gmond-3.1.1-1.s390x                                  3/3 
duration: 73(ms)
Installed products updated.

Installed:
  ganglia-gmetad.s390x 0:3.1.1-1          ganglia-gmond.s390x 0:3.1.1-1         
  ganglia-web.s390x 0:3.1.1-1            

Complete!</code></pre></li>
<li><p>Restart the HTTP server</p>
<pre><code># service httpd restart
Stopping httpd: [  OK  ]
Starting httpd: [  OK  ]</code></pre></li>
<li><p>Restart gmond and gmetad</p>
<pre><code># service gmetad restart
Shutting down GANGLIA gmetad: [FAILED]
Starting GANGLIA gmetad: [  OK  ]

# service gmond restart
Shutting down GANGLIA gmond: [FAILED]
Starting GANGLIA gmond: [  OK  ]</code></pre></li>
</ul></li>
<li><p>Create the directory /install/post/otherpkgs/&lt;os&gt;/s390x on the xCAT MN, where &lt;os&gt; is the SLES version you are running</p>
<pre><code># mkdir -p /install/post/otherpkgs/&amp;lt;os&amp;gt;/s390x</code></pre></li>
<li><p>Copy the following packages from /root/xcat/xcat-dep/&lt;os&gt;/s390x into /install/post/otherpkgs/&lt;os&gt;/s390x, where &lt;os&gt; is the SLES version you are running</p>
<pre><code>libganglia-3.1.1-1.s390x.rpm
libconfuse-2.6-1.s390x.rpm
ganglia-gmond-3.1.1-1.s390x.rpm</code></pre></li>
<li><p>Refer to <a href="https://sourceforge.net/apps/mediawiki/xcat/index.php?title=XCAT_zVM#Adding_Software_Packages">Adding Software Packages</a> section on how to automatically install Ganglia when provisioning nodes.</p></li>
</ol>
<h3 id="suse-linux-enterprise-server"><a href="#TOC">SUSE Linux Enterprise Server</a></h3>
<p>If you have SUSE Linux, follow the instructions below.</p>
<ol style="list-style-type: decimal">
<li>Logon the xCAT MN as root using a Putty terminal (if not already)</li>
<li><p>Go into the directory where you extracted the xcat-dep tarball, e.g. /root/xcat. Locate the Ganglia RPMs under /root/xcat/xcat-dep/&lt;os&gt;/s390x, where &lt;os&gt; is the SLES version you are running. Verify install the following RPMs are present.</p>
<pre><code># ls /root/xcat/xcat-dep/sles11/s390x
...
ganglia-devel-3.1.1-1.s390x.rpm
ganglia-gmetad-3.1.1-1.s390x.rpm
ganglia-gmond-3.1.1-1.s390x.rpm
ganglia-gmond-modules-python-3.1.1-1.s390x.rpm
ganglia-web-3.1.1-1.s390x.rpm
libconfuse-2.6-1.s390x.rpm
libconfuse-devel-2.6-1.s390x.rpm
libganglia-3.1.1-1.s390x.rpm
...</code></pre></li>
<li>Set up ganglia on the xCAT MN
<ul>
<li><p>Install PHP and apache packages (if not already). Use yast to install the following packages</p>
<pre><code># zypper install libapr1 pkgconfig php5-pear php5-gd apache2 apache2-mod_php5</code></pre></li>
<li><p>Install the Ganglia RPMs</p>
<pre><code># zypper install ganglia-gmetad ganglia-gmond ganglia-web
Loading repository data...
Reading installed packages...
Resolving package dependencies...

The following NEW packages are going to be installed:
  ganglia-gmetad ganglia-gmond ganglia-web libconfuse libganglia rrdtool 

The following packages are not supported by their vendor:
  ganglia-gmetad ganglia-gmond ganglia-web libconfuse libganglia 

6 new packages to install.
Overall download size: 981.0 KiB. After the operation, additional 3.9 MiB will 
be used.
Continue? [y/n/?] (y): y
Retrieving package rrdtool-1.3.4-2.8.s390x (1/6), 478.0 KiB (1.7 MiB unpacked)
Retrieving: rrdtool-1.3.4-2.8.s390x.rpm [done]
Installing: rrdtool-1.3.4-2.8 [done]
Retrieving package ganglia-web-3.1.1-1.s390x (2/6), 106.0 KiB (222.0 KiB unpacked)
Installing: ganglia-web-3.1.1-1 [done]
Retrieving package libconfuse-2.6-1.s390x (3/6), 102.0 KiB (468.0 KiB unpacked)
Installing: libconfuse-2.6-1 [done]
Retrieving package libganglia-3.1.1-1.s390x (4/6), 77.0 KiB (252.0 KiB unpacked)
Installing: libganglia-3.1.1-1 [done]
Retrieving package ganglia-gmetad-3.1.1-1.s390x (5/6), 67.0 KiB (188.0 KiB unpacked)
Installing: ganglia-gmetad-3.1.1-1 [done]
Additional rpm output:
gmetad                    0:off  1:off  2:off  3:on   4:off  5:on   6:off


Retrieving package ganglia-gmond-3.1.1-1.s390x (6/6), 151.0 KiB (1.1 MiB unpacked)
Installing: ganglia-gmond-3.1.1-1 [done]
Additional rpm output:
gmond                     0:off  1:off  2:off  3:on   4:off  5:on   6:off</code></pre></li>
<li><p>Restart the apache server</p>
<pre><code># service apache2 restart
Syntax OK
Shutting down httpd2 (waiting for all children to terminate)         done
Starting httpd2 (prefork)            </code></pre></li>
<li><p>Restart gmond and gmetad</p>
<pre><code># service gmond restart
Shutting down gmond                                                  done
Starting gmond                                                       done

# service gmetad restart
Shutting down gmetad                                                 done
Starting gmetad                                                      done</code></pre></li>
</ul></li>
<li><p>Create the directory /install/post/otherpkgs/&lt;os&gt;/s390x on the xCAT MN, where &lt;os&gt; is the SLES version you are running</p>
<pre><code># mkdir -p /install/post/otherpkgs/&amp;lt;os&amp;gt;/s390x</code></pre></li>
<li><p>Copy the following packages from /root/xcat/xcat-dep/&lt;os&gt;/s390x into /install/post/otherpkgs/&lt;os&gt;/s390x, where &lt;os&gt; is the SLES version you are running</p>
<pre><code>libganglia-3.1.1-1.s390x.rpm
libconfuse-2.6-1.s390x.rpm
ganglia-gmond-3.1.1-1.s390x.rpm</code></pre></li>
<li><p>Refer to <a href="https://sourceforge.net/apps/mediawiki/xcat/index.php?title=XCAT_zVM#Adding_Software_Packages">Adding Software Packages</a> section on how to automatically install Ganglia when provisioning nodes.</p></li>
</ol>
<h2 id="ganglia-monitoring-on-xcat"><a href="#TOC">Ganglia Monitoring on xCAT</a></h2>
<p>This section details how to use Ganglia on Linux on System z.</p>
<ol style="list-style-type: decimal">
<li>Logon the xCAT MN as root using a Putty terminal (if not already)</li>
<li><p>Transfer ganglia RPMs required to run gmond over to nodes you want to monitor</p>
<pre><code># xdcp &amp;lt;node&amp;gt; /install/post/otherpkgs/&amp;lt;os&amp;gt;/s390x/ganglia-gmond-3.1.1-1.s390x.rpm
# xdcp &amp;lt;node&amp;gt; /install/post/otherpkgs/&amp;lt;os&amp;gt;/s390x/libconfuse-2.6-1.s390x.rpm
# xdcp &amp;lt;node&amp;gt; /install/post/otherpkgs/&amp;lt;os&amp;gt;/s390x/libganglia-3.1.1-1.s390x.rpm</code></pre></li>
</ol>
<p>The command transfers the files into /root directory on the target nodes.</p>
<ol start="3" style="list-style-type: decimal">
<li><p>Install the RPMs</p>
<pre><code># xdsh &amp;lt;node&amp;gt; rpm -i libconfuse-2.6-1.s390x.rpm
# xdsh &amp;lt;node&amp;gt; rpm -i libganglia-3.1.1-1.s390x.rpm
# xdsh &amp;lt;node&amp;gt; rpm -i ganglia-gmond-3.1.1-1.s390x.rpm</code></pre></li>
</ol>
<p>Make sure the target node has <em>libapr1</em> (SLES) or <em>apr</em> (RHEL) package installed.</p>
<ol start="4" style="list-style-type: decimal">
<li><p>Ensure the nodetype of all nodes you wish to monitor have the type of <em>osi</em>. This can be done by editing the nodetype table.</p>
<pre><code># tabedit nodetype</code></pre></li>
<li><p>Add gangliamon to the monitoring table</p>
<pre><code># monadd gangliamon</code></pre></li>
<li><p>Configure the node</p>
<pre><code># moncfg gangliamon -r</code></pre></li>
</ol>
<p>This runs the ganglia configuration script on all the nodes.</p>
<ol start="7" style="list-style-type: decimal">
<li><p>If you want to start gangliamon:</p>
<pre><code># monstart gangliamon -r</code></pre></li>
</ol>
<p>The command will start the gmond daemon on all the nodes. The -r flag is required to ensure the gmond daemon is started on each node. You may also specify a particular node to start:</p>
<pre><code>    # monstart gangliamon gpok3 -r</code></pre>
<p>If you want to stop gangliamon on all nodes:</p>
<pre><code>    # monstop gangliamon -r</code></pre>
<h2 id="statelite"><a href="#TOC">Statelite</a></h2>
<p>This section details how to configure an NFS read-only root filesystem. For more details, refer to <a href="http://sourceforge.net/apps/mediawiki/xcat/index.php?title=XCAT_Linux_Statelite">xCAT Linux Statelite</a>. Note that you can only create statelite nodes that is of the same Linux distribution as your management node. For example, if your xCAT MN is SLES 11 SP1, you can only create SLES 11 SP1 statelite nodes.</p>
<h3 id="red-hat-enterprise-linux-1"><a href="#TOC">Red Hat Enterprise Linux</a></h3>
<p>If you have Red Hat Linux, follow the instructions below.</p>
<ol style="list-style-type: decimal">
<li>Logon the xCAT MN as root using a Putty terminal (if not already)</li>
<li><p>Edit /etc/exports to export the /install directory. It should look similar to this:</p>
<pre><code>/install *(rw,no_root_squash,sync,no_subtree_check)
/lite/state *(rw,no_root_squash,sync,no_subtree_check)</code></pre></li>
<li><p>Restart the NFS server</p>
<pre><code># service nfs restart</code></pre></li>
<li><p>Edit the litefile table. This table specifies which files should be kept persistent across reboots. By default, all files are kept under tmpfs, unless a persistent, ro, or bind option is specified. Refer to the litefile table description for more details.</p>
<pre><code># tabedit litefile</code></pre></li>
</ol>
<p>Copy the following defaults into the litefile table. This is the minimal list of files you need.</p>
<pre><code>    #image,file,options,comments,disable
    &quot;ALL&quot;,&quot;/etc/adjtime&quot;,,,
    &quot;ALL&quot;,&quot;/etc/fstab&quot;,,,
    &quot;ALL&quot;,&quot;/etc/lvm/&quot;,,,
    &quot;ALL&quot;,&quot;/etc/mtab&quot;,&quot;link&quot;,,
    &quot;ALL&quot;,&quot;/etc/syslog.conf&quot;,,,
    &quot;ALL&quot;,&quot;/etc/syslog.conf.XCATORIG&quot;,,,
    &quot;ALL&quot;,&quot;/etc/ntp.conf&quot;,,,
    &quot;ALL&quot;,&quot;/etc/ntp.conf.predhclient&quot;,,,
    &quot;ALL&quot;,&quot;/etc/resolv.conf&quot;,,,
    &quot;ALL&quot;,&quot;/etc/resolv.conf.predhclient&quot;,,,
    &quot;ALL&quot;,&quot;/etc/ssh/&quot;,&quot;persistent&quot;,,
    &quot;ALL&quot;,&quot;/etc/sysconfig/&quot;,,,
    &quot;ALL&quot;,&quot;/tmp/&quot;,,,
    &quot;ALL&quot;,&quot;/var/&quot;,,,
    &quot;ALL&quot;,&quot;/opt/xcat/&quot;,,,
    &quot;ALL&quot;,&quot;/xcatpost/&quot;,,,
    &quot;ALL&quot;,&quot;/root/.ssh/&quot;,,,</code></pre>
<ol start="5" style="list-style-type: decimal">
<li><p>Edit the litetree table. This table controls where the files specified in the litefile table come from.</p>
<pre><code># tabedit litetree</code></pre></li>
</ol>
<p>Copy the following into the litetree table. You will need to determine the Linux distribution you want. In our example, RHEL 5.4 is used.</p>
<pre><code>    #priority,image,directory,comments,disable
    &quot;1.0&quot;,,&quot;10.1.100.1:/install/netboot/rhel5.4/s390x/compute&quot;,,</code></pre>
<ol start="6" style="list-style-type: decimal">
<li><p>Edit the statelite table. This table controls where the permanent files are kept.</p>
<pre><code># tabedit statelite</code></pre></li>
</ol>
<p>Copy the following into the statelite table. You will need to determine the statelite node range and the IP address of the xCAT MN. In our example, the node range is <em>all</em> and the IP address is <em>10.1.100.1</em>.</p>
<pre><code>    #node,image,statemnt,comments,disable
    &quot;all&quot;,,&quot;10.1.100.1:/lite/state&quot;,,</code></pre>
<ol start="7" style="list-style-type: decimal">
<li><p>Create the persistent directory</p>
<pre><code># mkdir -p /lite/state</code></pre></li>
<li><p>Ensure policies are set up correctly. When a node boots up, it queries the xCAT database to get the lite-files and the lite-tree. In order for this to work, the command must be set in the policy table to allow nodes to request it. (This should already be done automatically when xCAT was installed)</p>
<pre><code># chtab priority=4.7 policy.commands=litefile policy.rule=allow
# chtab priority=4.8 policy.commands=litetree policy.rule=allow</code></pre></li>
<li><p>Download and copy the packages from the Linux distro media into /install (if not already)</p>
<pre><code># copycds -n xxx -a s390x /install/yyy.iso</code></pre></li>
</ol>
<p>Substitute xxx with the distribution name and yyy with the ISO name.</p>
<p>For example, if you have a RHEL 5.4 ISO:</p>
<pre><code>    # copycds -n rhel5.4 -a s390x /install/RHEL5.4-Server-20090819.0-s390x-DVD.iso</code></pre>
<ol start="10" style="list-style-type: decimal">
<li><p>Create a list of packages that should be installed onto the statelite image. You should start with the base packages in the compute template and if desired, add more packages by editing the .pkglist.</p>
<pre><code># mkdir -p /install/custom/netboot/rh
# cp /opt/xcat/share/xcat/netboot/sles/compute.rhe5.s390x.pkglist</code></pre></li>
<li><p>Create the statelite image</p>
<pre><code># genimage -i eth1 -n qeth -o rhel5.4 -p compute
OS: rhel5.4
Profile: compute
Interface: eth1
Network drivers: qeth
Do you need to set up other interfaces? [y/n] n
Which kernel do you want to use? [default] [Enter]</code></pre></li>
</ol>
<p>This command creates a <em>RHEL 5.4</em> image with an <em>eth1</em> interface, <em>qeth</em> network driver, and uses the <em>compute</em> profile. The interface used must match the xCAT MN interface that DHCP listens on. The genimage command creates an image under /install/netboot/rhel5.4/s390x/compute/rootimg. It also creates a ramdisk and kernel that is used to boot the statelite node.</p>
<ol start="12" style="list-style-type: decimal">
<li><p>Modify the statelite image by creating symbolic links with all the files listed under the litetree table</p>
<pre><code># liteimg -o rhel5.4 -a s390x -p compute
going to modify /install/netboot/rhel5.4/s390x/compute/rootimg
creating /install/netboot/rhel5.4/s390x/compute/rootimg/.statelite</code></pre></li>
<li><p>Create the statelite node definition.</p></li>
</ol>
<p>For our example, we will create a new node (gpok6) with a userID (LINUX6) that is managed by our zHCP (gpok2). You will need to substitute the node names, userIDs, and zHCP name with appropriate values.</p>
<pre><code>    # mkdef -t node -o gpok6 userid=LINUX6 hcp=gpok2.endicott.ibm.com mgt=zvm groups=all</code></pre>
<ol start="14" style="list-style-type: decimal">
<li><p>Update /etc/hosts</p>
<pre><code># makehosts</code></pre></li>
<li><p>Update DNS</p>
<pre><code># makedns</code></pre></li>
<li><p>Create the new virtual machine using the desired directory entry. For our example, we used the following:</p>
<pre><code>USER LNX6 PWD 512M 1G G
COMMAND SET VSWITCH VSW2 GRANT LNX6
CPU 00 BASE
CPU 01
IPL CMS
MACHINE ESA 4
CONSOLE 0009 3215 T
NICDEF 0800 TYPE QDIO LAN SYSTEM VSW2
SPOOL 000C 2540 READER *
SPOOL 000D 2540 PUNCH A
SPOOL 000E 1403 A
LINK MAINT 0190 0190 RR
LINK MAINT 019D 019D RR
LINK MAINT 019E 019E RR</code></pre></li>
</ol>
<p>To create the virtual server, copy the directory entry above into a text file (dirEntry.txt) and issue the following command (the full file path must be given):</p>
<pre><code>    # mkvm gpok6 /tmp/dirEntry.txt</code></pre>
<p>The new virtual server should be attached to the same VSWITCH as the one used by the hardware control point (in our case, VSW2) and have the same network adapter address (in our case, 0800) for the interface given in step 12 (in our case, eth1).</p>
<ol start="17" style="list-style-type: decimal">
<li><p>Add the new node to DHCP</p>
<pre><code># makedhcp -a</code></pre></li>
<li><p>Set up the noderes and nodetype tables. The values of nodetype.os and nodetype.profile were determined in step 11, where the statelite image was created.</p>
<pre><code># chtab node=xxx noderes.netboot=zvm nodetype.os=yyy nodetype.arch=s390x nodetype.profile=zzz</code></pre></li>
</ol>
<p>Substitute xxx with the node name, yyy with the operating system, and zzz with the profile name.</p>
<p>In our example, we used the following:</p>
<pre><code>    # chtab node=gpok6 noderes.netboot=zvm nodetype.os=rhel5.4 nodetype.arch=s390x nodetype.profile=compute</code></pre>
<ol start="19" style="list-style-type: decimal">
<li><p>Prepare the node(s) to boot from the statelite image</p>
<pre><code># nodeset xxx statelite</code></pre></li>
</ol>
<p>Substitute xxx is the node name.</p>
<ol start="20" style="list-style-type: decimal">
<li><p>Boot the statelite node(s). During this process, the symbolic links are made to files listed under the litefile table.</p>
<pre><code># rnetboot xxx ipl=00c</code></pre></li>
</ol>
<p>Substitute xxx is the node name.</p>
<p><strong>Caution</strong>: Do no try to boot more than 20 nodes at one time. The xCAT MN will be bogged down as all the nodes are trying to access the NFS server at once. Try booting 20 or less at a time and waiting till those nodes are pingable before booting the next batch.</p>
<ol start="21" style="list-style-type: decimal">
<li><p>Check the boot status of the node(s) by pinging them:</p>
<pre><code># pping xxx</code></pre></li>
</ol>
<p>Substitute xxx with the node name. If the node returns a ping, then it is fully booted and you can start using it.</p>
<ol start="22" style="list-style-type: decimal">
<li>Clone this node as many times as you want to achieve the number of statelite nodes you desire. Refer to <em>Cloning Virtual Servers</em> section above. In order to clone, the source statelite node must be online and have SSH keys setup. Once you have completed clonning, you will have to repeat steps 17 to 20 for all the cloned nodes.</li>
</ol>
<h3 id="suse-linux-enterprise-server-1"><a href="#TOC">SUSE Linux Enterprise Server</a></h3>
<p>If you have SUSE Linux, follow the instructions below.</p>
<ol style="list-style-type: decimal">
<li>Logon the xCAT MN as root using a Putty terminal (if not already)</li>
<li><p>Edit /etc/exports to export the /install directory. It should contain these two directories:</p>
<pre><code>/install *(rw,no_root_squash,sync,no_subtree_check)
/lite/state *(rw,no_root_squash,sync,no_subtree_check)</code></pre></li>
<li><p>Restart the NFS server</p>
<pre><code># service nfsserver restart</code></pre></li>
<li><p>Check that the NFS server is running</p>
<pre><code># rpcinfo -p</code></pre></li>
</ol>
<p>Make sure nfs is listed, e.g.</p>
<pre><code>    100003 2 tcp 2049 nfs
    100003 2 tcp 2049 nfs
    100003 3 tcp 2049 nfs
    100003 4 tcp 2049 nfs
    100003 2 udp 2049 nfs
    100003 3 udp 2049 nfs
    100003 4 udp 2049 nfs</code></pre>
<ol start="5" style="list-style-type: decimal">
<li><p>Edit the litefile table. This table specifies which files should be kept persistent across reboots. By default, all files are kept under tmpfs, unless a persistent, ro, or link option is specified. Refer to the litefile table description for more details.</p>
<pre><code># tabedit litefile</code></pre></li>
</ol>
<p>Copy the following defaults into the litefile table. This is the minimal list of files you need.</p>
<pre><code>    #image,file,options,comments,disable
    &quot;ALL&quot;,&quot;/etc/lvm/&quot;,,,
    &quot;ALL&quot;,&quot;/etc/mtab&quot;,&quot;link&quot;,,
    &quot;ALL&quot;,&quot;/etc/ntp.conf&quot;,,,
    &quot;ALL&quot;,&quot;/etc/ntp.conf.org&quot;,,,
    &quot;ALL&quot;,&quot;/etc/resolv.conf&quot;,,,
    &quot;ALL&quot;,&quot;/etc/ssh/&quot;,&quot;persistent&quot;,,
    &quot;ALL&quot;,&quot;/etc/sysconfig/&quot;,,,
    &quot;ALL&quot;,&quot;/etc/syslog-ng/&quot;,,,
    &quot;ALL&quot;,&quot;/tmp/&quot;,,,
    &quot;ALL&quot;,&quot;/var/&quot;,,,
    &quot;ALL&quot;,&quot;/etc/yp.conf&quot;,,,
    &quot;ALL&quot;,&quot;/etc/fstab&quot;,,,
    &quot;ALL&quot;,&quot;/opt/xcat/&quot;,,,
    &quot;ALL&quot;,&quot;/xcatpost/&quot;,,,
    &quot;ALL&quot;,&quot;/root/.ssh/&quot;,,,</code></pre>
<ol start="6" style="list-style-type: decimal">
<li><p>Edit the litetree table. This table controls where the files specified in the litefile table come from.</p>
<pre><code># tabedit litetree</code></pre></li>
<li><p>Copy the following into the litetree table. You will need to determine the Linux distribution you want. In our example, SLES11 SP1 is used.</p>
<pre><code>#priority,image,directory,comments,disable
&quot;1.0&quot;,,&quot;10.1.100.1:/install/netboot/sles11sp1/s390x/compute&quot;,,</code></pre></li>
<li><p>Edit the statelite table. This table controls where the permanent files are kept.</p>
<pre><code># tabedit statelite</code></pre></li>
</ol>
<p>Copy the following into the statelite table. You will need to determine the statelite node range and the IP address of the xCAT MN. In our example, the node range is <em>all</em> and the IP address is <em>10.1.100.1</em>.</p>
<pre><code>    #node,image,statemnt,comments,disable
    &quot;all&quot;,,&quot;10.1.100.1:/lite/state&quot;,,</code></pre>
<ol start="9" style="list-style-type: decimal">
<li><p>Create the persistent directory</p>
<pre><code># mkdir -p /lite/state</code></pre></li>
<li><p>Ensure policies are set up correctly. When a node boots up, it queries the xCAT database to get the lite-files and the lite-tree. In order for this to work, the command must be set in the policy table to allow nodes to request it. (This should already be done automatically when xCAT was installed)</p>
<pre><code># chtab priority=4.7 policy.commands=litefile policy.rule=allow
# chtab priority=4.8 policy.commands=litetree policy.rule=allow</code></pre></li>
<li><p>Download and copy the packages from the Linux distro media into /install (if not already)</p>
<pre><code># copycds -n xxx -a s390x /install/yyy.iso</code></pre></li>
</ol>
<p>Substitute xxx with the distribution name and yyy with the ISO name.</p>
<p>For example, if you have a SLES 11 SP1 ISO:</p>
<pre><code>    # copycds -n sles11sp1 -a s390x /install/SLES-11-SP1-DVD-s390x-GMC3-DVD1.iso</code></pre>
<ol start="12" style="list-style-type: decimal">
<li><p>Create a list of packages that should be installed onto the statelite image. You should start with the base packages in the compute template and if desired, add more packages by editing the .pkglist.</p>
<pre><code># mkdir -p /install/custom/netboot/sles
# cp /opt/xcat/share/xcat/netboot/sles/compute.sles11.s390x.pkglist /install/custom/netboot/sles</code></pre></li>
<li><p>Create the statelite image</p>
<pre><code># genimage -i eth1 -n qeth -o sles11sp1 -p compute
OS: sles11sp1
Profile: compute
Interface: eth1
Network drivers: qeth
Do you need to set up other interfaces? [y/n] n
Which kernel do you want to use? [default] [Enter]</code></pre></li>
</ol>
<p>This command creates a <em>SLES11 SP1</em> image with an <em>eth1</em> interface, <em>qeth</em> network driver, and uses the <em>compute</em> profile. The interface used must match the xCAT MN interface that DHCP listens on. The genimage command creates an image under /install/netboot/sles11sp1/s390x/compute/rootimg. It also creates a ramdisk and kernel that is used to boot the statelite node.</p>
<ol start="14" style="list-style-type: decimal">
<li><p>Modify the statelite image by creating symbolic links with all the files listed under the litetree table</p>
<pre><code># liteimg -o sles11sp1 -a s390x -p compute
going to modify /install/netboot/sles11sp1/s390x/compute/rootimg
creating /install/netboot/sles11sp1/s390x/compute/rootimg/.statelite</code></pre></li>
<li><p>Create the statelite node definition.<br />For our example, we will create a new node (gpok6) with a userID (LNX6) that is managed by our zHCP (gpok2). You will need to substitute the node names, userIDs, and zHCP name with appropriate values.</p>
<pre><code># mkdef -t node -o gpok6 userid=LINUX6 hcp=gpok2.endicott.ibm.com mgt=zvm groups=all</code></pre></li>
<li><p>Update /etc/hosts</p>
<pre><code># makehosts</code></pre></li>
<li><p>Update DNS</p>
<pre><code># makedns</code></pre></li>
<li><p>Create the new virtual server using the desired directory entry. For our example, we used the following:</p>
<pre><code>USER LNX6 PWD 512M 1G G
COMMAND SET VSWITCH VSW2 GRANT LNX6
CPU 00 BASE
CPU 01
IPL CMS
MACHINE ESA 4
CONSOLE 0009 3215 T
NICDEF 0800 TYPE QDIO LAN SYSTEM VSW2
SPOOL 000C 2540 READER *
SPOOL 000D 2540 PUNCH A
SPOOL 000E 1403 A
LINK MAINT 0190 0190 RR
LINK MAINT 019D 019D RR
LINK MAINT 019E 019E RR</code></pre></li>
</ol>
<p>To create the virtual server, copy the directory entry above into a text file (dirEntry.txt) and issue the following command (the full file path must be given):</p>
<pre><code>    # mkvm gpok6 /tmp/dirEntry.txt</code></pre>
<p>The new virtual server should be attached to the same vswitch as the one used by the hardware control point (in our case, VSW2) and have the same network adapter address (in our case, 0800) for the interface given in step 12 (in our case, eth1).</p>
<ol start="19" style="list-style-type: decimal">
<li><p>Add the new node to DHCP</p>
<pre><code># makedhcp -a</code></pre></li>
<li><p>Set up the noderes and nodetype tables. The values of nodetype.os and nodetype.profile were determined in step 11, where the statelite image was created.</p>
<pre><code># chtab node=xxx noderes.netboot=zvm nodetype.os=yyy nodetype.arch=s390x nodetype.profile=zzz</code></pre></li>
</ol>
<p>Substitute xxx with the node name, yyy with the operating system, and zzz with the profile name.</p>
<p>In our example, we used the following:</p>
<pre><code>    # chtab node=gpok6 noderes.netboot=zvm nodetype.os=sles11sp1 nodetype.arch=s390x nodetype.profile=compute</code></pre>
<ol start="21" style="list-style-type: decimal">
<li><p>Prepare the node(s) to boot from the statelite image</p>
<pre><code># nodeset xxx statelite</code></pre></li>
</ol>
<p>where xxx is the node name.</p>
<ol start="22" style="list-style-type: decimal">
<li><p>Boot the statelite node(s). During this process, the symbolic links are made to files listed under the litefile table.</p>
<pre><code># rnetboot xxx ipl=00c</code></pre></li>
</ol>
<p>where xxx is the node name.</p>
<p>Caution: Do no try to boot more than 20 nodes at one time. The xCAT MN will be bogged down as all the nodes are trying to access the NFS server at once. Try booting 20 or less at a time and waiting till those nodes are pingable before booting the next batch.</p>
<ol start="23" style="list-style-type: decimal">
<li><p>Check the boot status of the nodes by pinging them:</p>
<pre><code># pping xxx</code></pre></li>
</ol>
<p>Substitute xxx with the node name. If the node returns a ping, then it is fully booted and you can start using it.</p>
<ol start="24" style="list-style-type: decimal">
<li>Clone this node as many times as you want to achieve the number of statelite nodes you desire. Refer to <em>Cloning Virtual Servers</em> section above. In order to clone, the source statelite node must be online and have SSH keys setup. Once you have completed clonning, you will have to repeat steps 19 to 22 for all the cloned nodes.</li>
</ol>
<h2 id="updating-linux"><a href="#TOC">Updating Linux</a></h2>
<p>This section details how to update the Linux operating system.</p>
<ol style="list-style-type: decimal">
<li><p>Download and extract the ISO into the xCAT install tree /install (if not already)</p>
<pre><code># copycds -n xxx -a s390x /install/yyy.iso</code></pre></li>
</ol>
<p>Substitute xxx with the distribution name and yyy with the ISO name.</p>
<p>For example, if you have a SUSE Linux Enterprise Server 10 SP3 ISO:</p>
<pre><code>    # copycds -n sles10sp3 -a s390x /install/SLES-10-SP3-DVD-s390x-DVD1.iso
    Copying media to /install/sles10sp3/s390x/1
    Media copy operation successful</code></pre>
<p>or if you have a Red Hat Enterprise Linux 5.4 ISO:</p>
<pre><code>    # copycds -n rhel5.4 -a s390x /install/RHEL5.4-Server-20090819.0-s390x-DVD.iso
    Copying media to /install/rhel5.4/s390x
    Media copy operation successful</code></pre>
<ol start="2" style="list-style-type: decimal">
<li><p>Update the node</p>
<pre><code># updatenode xxx -o yyy</code></pre></li>
</ol>
<p>Substitute xxx with the node name and yyy with the operating system version.</p>
<p>For example, if you want to update gpok5 to RHEL5.4 (assuming gpok5 has RHEL 5.3):</p>
<pre><code>    # updatenode gpok5 -o rhel5.4</code></pre>
<p>The command requires the node to be online. It will take several minutes to complete the update. You can only update to the next release. For example, you can only update RHEL5.3 to RHEL5.4. You cannot skip releases, e.g. updating RHEL5.3 to RHEL5.5.</p>
<p><strong>Warning</strong>: You cannot update SLES10.3 to SLES11. There is a bug in <code>rug</code> where you cannot add a repository/service.</p>
<h2 id="limitations"><a href="#TOC">Limitations</a></h2>
<p>This section highlights the limitations of xCAT on z/VM and Linux on System z.</p>
<ol style="list-style-type: decimal">
<li>xCAT is only supported on z/VM 5.4 or newer.</li>
<li>zHCP is only supported on RHEL 5.4 or newer, and SLES 10 SP2 or newer.</li>
<li>The default autoyast and kickstart templates available on xCAT was tested on SLES 10.2/10.3/11/11.1/11.2 and RHEL 5.3/5.4/5.5/6.0/6.1/6.2.</li>
<li>Cloning LVM volumes is supported. However, it is not supported on nodes where the root file system is on an LVM volume.</li>
<li>CP Flashcopy is only supported on ECKD volumes. These volumes must be on the same storage facility.</li>
<li>Statelite is only supported on SLES 11 or newer, and RHEL 5.4 or newer.</li>
<li>Nodes that the zHCP manages must have the Linux VMCP module.</li>
<li>A layer 2 VSWITCH is required for DHCP.</li>
<li>In order for the xCAT MN to manage across multiple LPARs and CECs, you must use a layer 2 VSWITCH. The network hardware must be configured in such a way that these VSWITCHes can communicate across multiple LPARs and CECs.</li>
</ol>
<h2 id="appendix-a-setting-up-a-second-network"><a href="#TOC">Appendix A: Setting Up a Second Network</a></h2>
<p>This section details how to setup a second network based on a layer 2 VSWITCH.</p>
<h3 id="red-hat-enterprise-linux-2"><a href="#TOC">Red Hat Enterprise Linux</a></h3>
<p>SSH to the desire Linux where you want to setup the private network. A network script must be added under /etc/sysconfig/network-scripts/ to let the system know about the new interface and a qeth group must be created under /sys/bus/ccwgroup/drivers/qeth/group.</p>
<p>In the following example, we will configure an ethernet interface (<em>eth1</em>) for a layer 2 VSWITCH (<em>VSW2</em>) attached to <em>0800</em>. We will assume there is an existing ethernet interface (<em>eth0</em>) for a network card attached to <em>0600</em>.</p>
<p>Copy the hardware settings from the existing network /etc/sysconfig/network-scripts/ifcfg-eth0.</p>
<pre><code># cp /etc/sysconfig/network-scripts/ifcfg-eth0 /etc/sysconfig/network-scripts/ifcfg-eth1</code></pre>
<p>Edit the network settings.</p>
<pre><code># vi /etc/sysconfig/network-scripts/ifcfg-eth1</code></pre>
<p>It should look similar to the following:</p>
<pre><code># IBM QETH
DEVICE=eth1
ARP=no
BOOTPROTO=static
BROADCAST=10.1.100.255
IPADDR=10.1.100.1
IPV6INIT=yes
IPV6_AUTOCONF=yes
MTU=1500
NETMASK=255.255.255.0
NETTYPE=qeth
NETWORK=10.1.100.0
ONBOOT=yes
PORTNAME=PORT800
OPTIONS=&quot;layer2=1&quot;
SUBCHANNELS=0.0.0800,0.0.0801,0.0.0802</code></pre>
<p>You need to substitute the broadcast, IP address, netmask, network, port name, and subchannels with appropriate values. If you have a layer 3 device, set OPTIONS=&quot;layer2=0&quot;.</p>
<p>Load the qeth driver</p>
<pre><code># modprobe qeth</code></pre>
<p>Create a qeth group device</p>
<pre><code># echo 0.0.0800,0.0.0801,0.0.0802 &amp;gt; /sys/bus/ccwgroup/drivers/qeth/group</code></pre>
<p>Declare the qeth group device as Layer 2</p>
<pre><code># echo 1 &amp;gt; /sys/bus/ccwgroup/drivers/qeth/0.0.0800/layer2</code></pre>
<p>Bring the device back online (you need to reset the device after each reboot)</p>
<pre><code># echo 1 &amp;gt; /sys/bus/ccwgroup/drivers/qeth/0.0.0800/online</code></pre>
<p>Verify the state of the device (1 = online)</p>
<pre><code># cat /sys/bus/ccwgroup/drivers/qeth/0.0.0800/online</code></pre>
<p>Check to see what interface name was assigned to the device</p>
<pre><code># cat /sys/bus/ccwgroup/drivers/qeth/0.0.0800/if_name</code></pre>
<p>A qeth device requires an alias definition in /etc/modprobe.conf. Edit this file and add an alias for your interface. (This action is not necessary in RHEL 6)</p>
<pre><code># vi /etc/modprobe.conf


alias eth0 qeth
alias eth1 qeth
options dasd_mod dasd=0.0.0100,0.0.0103,0.0.0300,0.0.0301</code></pre>
<p>Start the new interface</p>
<pre><code># ifup eth1</code></pre>
<h3 id="suse-linux-enterprise-server-10"><a href="#TOC">SUSE Linux Enterprise Server 10</a></h3>
<p>SSH to the desire Linux where you want to setup the private network. Two configuration files must be added under /etc/sysconfig/ to let the system know about the new interface, one for hardware and one for network settings.</p>
<p>In the following example, we will configure an ethernet interface (<em>eth1</em>) for a layer 2 VSWITCH (<em>VSW2</em>) attached to <em>0800</em>. We will assume there is an existing ethernet interface (<em>eth0</em>) for a network card attached to <em>0600</em>.</p>
<p>Copy the hardware settings from the existing network /etc/sysconfig/hardware/hwcfg-qeth-bus-ccw-0.0.0600. Both interfaces will use the qdio/qeth drivers, therefore, the configuration files can be identical except for the virtual addresses. The existing file is copied to specify the new NIC. The only difference needed is to change the <em>060X</em> values to <em>080X</em>.</p>
<pre><code># cd /etc/sysconfig/hardware/</code></pre>
<p>Edit the hardware settings.</p>
<pre><code># sed *600 -e &#39;s/060/080/g&#39; &amp;gt; hwcfg-qeth-bus-ccw-0.0.0800</code></pre>
<p>It should look similar to the following:</p>
<pre><code>STARTMODE=&quot;auto&quot;
MODULE=&quot;qeth&quot;
MODULE_OPTIONS=&quot;&quot;
MODULE_UNLOAD=&quot;yes&quot;
SCRIPTUP=&quot;hwup-ccw&quot;
SCRIPTUP_ccw=&quot;hwup-ccw&quot;
SCRIPTUP_ccwgroup=&quot;hwup-qeth&quot;
SCRIPTDOWN=&quot;hwdown-ccw&quot;
CCW_CHAN_IDS=&quot;0.0.0800 0.0.0801 0.0.0802&quot;
CCW_CHAN_NUM=&quot;3&quot;
CCW_CHAN_MODE=&quot;OSAPORT&quot;
QETH_LAYER2_SUPPORT=&quot;1&quot;</code></pre>
<p>Copy the network settings from the existing network /etc/sysconfig/network/ifcfg-qeth-bus-ccw-0.0.0600.</p>
<pre><code># cd /etc/sysconfig/network
# cp ifcfg-qeth-bus-ccw-0.0.0600 ifcfg-qeth-bus-ccw-0.0.0800</code></pre>
<p>Edit the network settings.</p>
<pre><code># vi ifcfg-qeth-bus-ccw-0.0.0800</code></pre>
<p>It should look similar to the following:</p>
<pre><code>BOOTPROTO=&quot;static&quot;
UNIQUE=&quot;&quot;
STARTMODE=&quot;onboot&quot;
IPADDR=&quot;10.1.100.1&quot;
NETMASK=&quot;255.255.255.0&quot;
NETWORK=&quot;10.1.100.0&quot;
BROADCAST=&quot;10.1.100.255&quot;
_nm_name=&#39;qeth-bus-ccw-0.0.0800&#39;</code></pre>
<p>You need to substitute the broadcast, IP address, netmask, and network with appropriate values.</p>
<p>Reboot the virtual server to have the changes take effect.</p>
<pre><code># reboot</code></pre>
<h3 id="suse-linux-enterprise-server-11"><a href="#TOC">SUSE Linux Enterprise Server 11</a></h3>
<p>SSH to the desire Linux where you want to setup the private network. A configuration file must be added under /etc/sysconfig/network and /etc/udev/rules.d to let the system know about the new interface.</p>
<p>In the following example, we will configure an ethernet interface (<em>eth1</em>) for a layer 2 VSWITCH (<em>VSW2</em>) attached to <em>0800</em>. We will assume there is an existing ethernet interface (<em>eth0</em>) for a network card attached to <em>0600</em>.</p>
<p>Copy the hardware settings from the existing network /etc/udev/rules.d/51-qeth-0.0.0600.rules. Both interfaces will use the qdio/qeth drivers, therefore, the configuration files can be identical except for the virtual addresses. The existing file is copied to specify the new NIC. The only difference needed is to change the <em>060X</em> values to <em>080X</em>.</p>
<pre><code># sed /etc/udev/rules.d/51-qeth-0.0.0600.rules -e &#39;s/060/080/g&#39; &amp;gt; /etc/udev/rules.d/51-qeth-0.0.0800.rules</code></pre>
<p>Edit the udev rules</p>
<pre><code># vi /etc/udev/rules.d/51-qeth-0.0.0800.rules</code></pre>
<p>It should look similar to the following:</p>
<pre><code># Configure qeth device at 0.0.0800/0.0.0801/0.0.0802
ACTION==&quot;add&quot;, SUBSYSTEM==&quot;drivers&quot;, KERNEL==&quot;qeth&quot;, IMPORT{program}=&quot;collect 0.0.0800 %k 0.0.0800 0.0.0801 0.0.0802 qeth&quot;
ACTION==&quot;add&quot;, SUBSYSTEM==&quot;ccw&quot;, KERNEL==&quot;0.0.0800&quot;, IMPORT{program}=&quot;collect 0.0.0800 %k 0.0.0800 0.0.0801 0.0.0802 qeth&quot;
ACTION==&quot;add&quot;, SUBSYSTEM==&quot;ccw&quot;, KERNEL==&quot;0.0.0801&quot;, IMPORT{program}=&quot;collect 0.0.0800 %k 0.0.0800 0.0.0801 0.0.0802 qeth&quot;
ACTION==&quot;add&quot;, SUBSYSTEM==&quot;ccw&quot;, KERNEL==&quot;0.0.0802&quot;, IMPORT{program}=&quot;collect 0.0.0800 %k 0.0.0800 0.0.0801 0.0.0802 qeth&quot; TEST==&quot;[ccwgroup/0.0.0800]&quot;, GOTO=&quot;qeth-0.0.0800-end&quot;
ACTION==&quot;add&quot;, SUBSYSTEM==&quot;ccw&quot;, ENV{COLLECT_0.0.0800}==&quot;0&quot;, ATTR{[drivers/ccwgroup:qeth]group}=&quot;0.0.0800,0.0.0801,0.0.0802&quot;
ACTION==&quot;add&quot;, SUBSYSTEM==&quot;drivers&quot;, KERNEL==&quot;qeth&quot;, ENV{COLLECT_0.0.0800}==&quot;0&quot;, ATTR{[drivers/ccwgroup:qeth]group}=&quot;0.0.0800,0.0.0801,0.0.0802&quot; LABEL=&quot;qeth-0.0.0800-end&quot;
ACTION==&quot;add&quot;, SUBSYSTEM==&quot;ccwgroup&quot;, KERNEL==&quot;0.0.0800&quot;, ATTR{portname}=&quot;OSAPORT&quot;
ACTION==&quot;add&quot;, SUBSYSTEM==&quot;ccwgroup&quot;, KERNEL==&quot;0.0.0800&quot;, ATTR{portno}=&quot;0&quot;
ACTION==&quot;add&quot;, SUBSYSTEM==&quot;ccwgroup&quot;, KERNEL==&quot;0.0.0800&quot;, ATTR{layer2}=&quot;1&quot;
ACTION==&quot;add&quot;, SUBSYSTEM==&quot;ccwgroup&quot;, KERNEL==&quot;0.0.0800&quot;, ATTR{online}=&quot;1&quot;</code></pre>
<p>You must also enable layer2 for the device. Take note of <code>ATTR{layer2}=&quot;1&quot;</code>.</p>
<p>Copy the network settings from the existing network /etc/sysconfig/network/ifcfg-eth0.</p>
<pre><code># cp /etc/sysconfig/network/ifcfg-eth0 /etc/sysconfig/network/ifcfg-eth1</code></pre>
<p>Edit the network settings.</p>
<pre><code># vi /etc/sysconfig/network/ifcfg-eth1</code></pre>
<p>It should look similar to the following:</p>
<pre><code>BOOTPROTO=&#39;static&#39;
IPADDR=&#39;10.1.100.1&#39;
BROADCAST=&#39;10.1.100.255&#39;
NETMASK=&#39;255.255.255.0&#39;
NETWORK=&#39;10.1.100.0&#39;
STARTMODE=&#39;onboot&#39;
NAME=&#39;OSA Express Network card (0.0.0800)&#39;</code></pre>
<p>Reboot the virtual server to have the changes take effect.</p>
<pre><code># reboot</code></pre>
<h2 id="appendix-b-customizing-autoyast-and-kickstart"><a href="#TOC">Appendix B: Customizing Autoyast and Kickstart</a></h2>
<p>This section details how to customize the autoyast and kickstart templates. It should only serve as a quick guide on configuring the templates. It is beyond the scope of this document to go into details on configuring autoyast and kickstart. You need to go to the links provided below to get more information.</p>
<p>Autoyast and kickstart allows you to customize a Linux system based on a template. While you would typically go through various panels to manually customize your Linux system during boot, you no longer have to with autoyast and kickstart. This allows you to configure a vanilla Linux system faster and more effectively.</p>
<h3 id="red-hat-enterprise-server"><a href="#TOC">Red Hat Enterprise Server</a></h3>
<ol style="list-style-type: decimal">
<li>Base your customization on the default template (compute.rhel5.s390x.tmpl) in /opt/xcat/share/xcat/install/rh/. This template is configured to setup the network for you using DHCP.</li>
<li>Determine the number of disks (ECKD or SCSI) your vanilla system will have and the mount points for each disk. Note that there are no extra steps needed to specify the disk type.</li>
<li><p>Copy the default template /opt/xcat/share/xcat/install/rh/xxx.tmpl, where xxx is the template name, into /install/custom/install/rh/. For our example, we will use compute.rhel5.s390x.tmpl:</p>
<pre><code># cp /opt/xcat/share/xcat/install/rh/compute.rhel5.s390x.tmpl /install/custom/install/rh/custom.rhel5.s390x.tmpl</code></pre></li>
</ol>
<p>The default templates are configured to use one 3390-mod9 with the root filesystem (/) mounted, install the base software package, and use DHCP. You can use it as a starting point and customize the disks, partitioning, install packages, and network configuration.</p>
<ol start="4" style="list-style-type: decimal">
<li><p>Add this template to the osimage table. For our example, we customized the kickstart template for RHEL 5.4 and added it to the osimage table using:</p>
<pre><code># chtab imagename=rhel5.4-s390x-install-custom osimage.profile=custom osimage.imagetype=linux osimage.provmethod=install osimage.osname=Linux osimage.osvers=rhel5.4 osimage.osarch=s390x</code></pre></li>
<li><p>Add the disk and mount point to the template using the following format:</p>
<pre><code>clearpart --initlabel --drives=dasda,dasdb
part / --fstype ext3 --size=100 --grow  --ondisk=dasda
part /usr --fstype ext3 --size=100 --grow  --ondisk=dasdb</code></pre></li>
</ol>
<p>In the example above, a disk is added with a device name of <em>dasdb</em>. The disk will be mounted at <em>/usr</em> and will have a <em>ext3</em> file system.</p>
<ol start="6" style="list-style-type: decimal">
<li><p>If you want to assign a static IP address to the system, edit the network option so that it is similar to the one below.</p>
<pre><code>network --bootproto=static --ip=replace_ip --netmask=replace_netmask --gateway=replace_gateway --nameserver=replace_nameserver --hostname=replace_hostname</code></pre></li>
</ol>
<p>During nodeset, xCAT will replace the placeholders (replace_*) with the appropriate values for the system.</p>
<ol start="7" style="list-style-type: decimal">
<li>Add the software you need to the <code>%packages</code> section.</li>
</ol>
<p>For more information, refer to <a href="http://docs.redhat.com/docs/en-US/Red_Hat_Enterprise_Linux/6/html/Installation_Guide/ch-kickstart2.html">Red Hat Enterprise Linux Installation Guide</a>.</p>
<h3 id="suse-linux-enterprise-server-2"><a href="#TOC">SUSE Linux Enterprise Server</a></h3>
<p>An autoyast generator is available for SLES 10 and SLES 11. It will help create an autoyast template with the desired DASD, partition layout, and software. It will create an autoyast template that can be consumed by xCAT. For more advance configurations (e.g. LDAP), the autoyast template has to be configured manually.</p>
<p>To generate an autoyast template:</p>
<ol style="list-style-type: decimal">
<li><p>Run mkay4z script under /opt/xcat/share/xcat/tools</p>
<pre><code># /opt/xcat/share/xcat/tools/mkay4z

Creating autoyast template for Linux on System z...
Select SUSE Linux Enterprise Server version? (10 or 11) 11
Where do you want to place the template? (e.g. /tmp/custom.sles11.s390x.tmpl) /tmp/custom.sles11.s390x.tmpl
  Do you want to use DHCP? (yes or no) y

CONFIGURING DASD...
Select from the following options:
  (1) Add DASD
  (2) Remove DASD
  (3) Show DASD configuration
  (4) Go to next step
1
  What is the virtual address? 100
  What is the type? (eckd or fba) eckd
Select from the following options:
  (1) Add DASD
  (2) Remove DASD
  (3) Show DASD configuration
  (4) Go to next step
1
  What is the virtual address? 101
  What is the type? (eckd or fba) eckd
Select from the following options:
  (1) Add DASD
  (2) Remove DASD
  (3) Show DASD configuration
  (4) Go to next step
4

CONFIGURING PARTITIONS...
Select a device from the list below to create a new partition.
#  |   Device   |   Address   |   Type   


* * *


0   /dev/dasda   0.0.0100      dasd_eckd_mod
1   /dev/dasdb   0.0.0101      dasd_eckd_mod
Which device do you want to configure? (See list above)
Leave blank and hit Enter to go to next step.
0
  What is the filesystem for /dev/dasda? (ext2, ext3, ext4, or swap) ext4
  What is the partition size? (e.g. 1g, 2g, or max) max
  Do you want to assign it to an LVM group? (yes or no) n
  What is the mount point? /
Which device do you want to configure? (See list above)
Leave blank and hit Enter to go to next step.
1
  What is the filesystem for /dev/dasdb? (ext2, ext3, ext4, or swap) ext4
  What is the partition size? (e.g. 1g, 2g, or max) max
  Do you want to assign it to an LVM group? (yes or no) n
  What is the mount point? /opt
Which device do you want to configure? (See list above)
Leave blank and hit Enter to go to next step.

Done! See autoyast template under /tmp/custom.sles11.s390x.tmpl</code></pre></li>
</ol>
<p>The script will ask you several questions concerning the configuration in the autoyast template. It is designed to help you configure the disks, partitions, and networking in the autoyast template for xCAT. It is important to note that the template name is significant. The name should be in the following order: &lt;profile&gt;.&lt;osvers&gt;.s390x.tmpl. For more advanced configurations, you should manually edit the autoyast template.</p>
<ol start="2" style="list-style-type: decimal">
<li><p>Place the custom template generated by the mkay4z script under /install/custom/install/sles/.</p>
<pre><code># mv /tmp/custom.sles11.s390x.tmpl /install/custom/install/sles/</code></pre></li>
<li><p>The custom template will need an associated package list. Copy an existing package list, e.g. compute.sles11.s390x.pkglist, and make appropriate modifications.</p>
<pre><code># cp /install/custom/install/sles/compute.sles11.s390x.pkglist /install/custom/install/sles/custom.sles11.s390x.pkglist</code></pre></li>
</ol>
<p>Note that the name of the package list must match the template profile name.</p>
<ol start="4" style="list-style-type: decimal">
<li><p>Add this template to the osimage table. For our example, we customized the autoyast template for SLES 11 SP1 and added it to the osimage table using:</p>
<pre><code># chtab imagename=sles11sp1-s390x-install-custom osimage.profile=custom osimage.imagetype=linux osimage.provmethod=install osimage.osname=Linux osimage.osvers=sles11sp1 osimage.osarch=s390x</code></pre></li>
</ol>
<p>For more information, refer to <a href="http://www.suse.de/~ug/autoyast_doc/index.html">openSUSE AutoYast</a>.</p>
<h2 id="appendix-c-setting-up-network-address-translation"><a href="#TOC">Appendix C: Setting up Network Address Translation</a></h2>
<p>This section details how to setup network address translation (NAT) on a Linux host. NAT supports both layer 2 and 3 network devices. The setup below uses iptables and port forwarding to allow hosts on a private network to gain access to a public network. It is important to note that the Linux host must have both external (public) and internal (private) interfaces. NAT will route packets appropriately between the public and private networks using iptables. It is also important to note that a host on the private network cannot be reached via the public network because it does not have a unique public IP address. However, this can be solved by assigning a unique port number on the Linux host (setup with NAT), so that packets sent to this port will be forwarding to the private host.</p>
<h3 id="red-hat-enterprise-server-1"><a href="#TOC">Red Hat Enterprise Server</a></h3>
<p>This section details how to setup NAT on Red Hat Enterprise Server. It is assumed that the Linux host (10.1.100.1) already has both external and internal interfaces. The external interface (eth0) is on the 9.10.11.0/24 network. The internal interface (eth1) is on the 10.1.100.0/24 network.</p>
<ol style="list-style-type: decimal">
<li><p>Allow forwarding for the internal interface (eth1)</p>
<pre><code># iptables --append FORWARD --in-interface eth1 -j ACCEPT</code></pre></li>
<li><p>Allow hosts on the private network to mask requests using the public IP address of the Linux host</p>
<pre><code># iptables --table nat --append POSTROUTING --out-interface eth0 -j MASQUERADE </code></pre></li>
<li><p>Edit /etc/sysctl.conf and enable IP forwarding with the following setting</p>
<pre><code>net.ipv4.ip_forward = 1</code></pre></li>
<li><p>Update the system configuration to enable IP forwarding</p>
<pre><code># sysctl -p /etc/sysctl.conf
net.ipv4.ip_forward = 1
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.default.accept_source_route = 0</code></pre></li>
<li><p>Allow appropriate services through the firewall. For example, allow SSH (port 22) through the firewall.</p>
<pre><code># iptables -A INPUT -p tcp -m state --state NEW -m tcp --dport 22 -j ACCEPT </code></pre></li>
<li><p>If you want a host (10.1.100.123) on the private network to be accessed publicly via SSH (port 22), you can forwarding the SSH request with the following command</p>
<pre><code># iptables -A PREROUTING -i eth+ -p tcp -m tcp --dport 2123 -j DNAT --to-destination 10.1.100.123:22
# iptables -A FORWARD -d 10.1.100.123/32 -i eth+ -p tcp -m state --state NEW -m tcp --dport 22 -j ACCEPT</code></pre></li>
</ol>
<p>Any request coming through port 2123 will be forwarded to the host on port 22 on the private network. It is important to note that the port number used must be free and not in use by any other service.</p>
<ol start="7" style="list-style-type: decimal">
<li><p>Publicly access the host on the private network</p>
<pre><code># ssh root@10.1.100.1 -p 2123</code></pre></li>
<li><p>Verify the NAT configuration by logging into a host (10.1.100.123) on the private network and accessing an external site from that host</p>
<pre><code># ifconfig
eth0      Link encap:Ethernet  HWaddr 02:00:01:FF:FE:FD  
          inet addr:10.1.100.123  Bcast:10.1.100.255  Mask:255.255.255.0
          inet6 addr: fd55:faaf:e1ab:263:0:6ff:feff:fefd/64 Scope:Global
          inet6 addr: fe80::6ff:feff:fefd/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1492  Metric:1
          RX packets:261657 errors:0 dropped:0 overruns:0 frame:0
          TX packets:314748 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000 
          RX bytes:103074088 (98.2 Mb)  TX bytes:27570328 (26.2 Mb)

lo        Link encap:Local Loopback  
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:16436  Metric:1
          RX packets:29 errors:0 dropped:0 overruns:0 frame:0
          TX packets:29 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0 
          RX bytes:7060 (6.8 Kb)  TX bytes:7060 (6.8 Kb)

# ping -c 4 sourceforge.net
PING sourceforge.net (216.34.181.60) 56(84) bytes of data.
64 bytes from ch3.sourceforge.net (216.34.181.60): icmp_seq=1 ttl=236 time=30.2 ms
64 bytes from ch3.sourceforge.net (216.34.181.60): icmp_seq=2 ttl=236 time=30.6 ms
64 bytes from ch3.sourceforge.net (216.34.181.60): icmp_seq=3 ttl=236 time=30.0 ms
64 bytes from ch3.sourceforge.net (216.34.181.60): icmp_seq=4 ttl=236 time=29.9 ms

--- sourceforge.net ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 3004ms
rtt min/avg/max/mdev = 29.908/30.224/30.666/0.357 ms</code></pre></li>
</ol>
<p>If the host cannot reach the external site, make sure that the gateway for the default route goes to the Linux host (10.1.100.1/gpok1) you had setup with NAT.</p>
<pre><code>    # route
    Kernel IP routing table
    Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
    10.1.100.0      *               255.255.255.0   U     0      0        0 eth0
    link-local      *               255.255.0.0     U     0      0        0 eth0
    loopback        *               255.0.0.0       U     0      0        0 lo
    default         gpok1.endicott. 0.0.0.0         UG    0      0        0 eth0</code></pre>
<p>For more information, refer to <a href="http://docs.redhat.com/docs/en-US/Red_Hat_Enterprise_Linux/6/html/Security_Guide/sect-Security_Guide-Firewalls-FORWARD_and_NAT_Rules.html">Red Hat Enterprise Linux 6 Security Guide</a>.</p>
<h3 id="suse-linux-enterprise-server-3"><a href="#TOC">SUSE Linux Enterprise Server</a></h3>
<p>This section details how to setup NAT on SUSE Linux Enterprise Server. It is assumed that the Linux host (10.1.100.1) already has both external and internal interfaces. The external interface (eth0) is on the 9.10.11.0/24 network. The internal interface (eth1) is on the 10.1.100.0/24 network.</p>
<ol style="list-style-type: decimal">
<li><p>Enable the SuSEfirewall2 boot scripts</p>
<pre><code># chkconfig SuSEfirewall2_init on
# chkconfig SuSEfirewall2_setup on</code></pre></li>
<li><p>Edit /etc/sysconfig/SuSEfirewall2 such that it contains the following configurations</p>
<pre><code># Space separated interfaces that point to the internet
FW_DEV_EXT=&quot;any eth0&quot;

# Space separated interfaces that point to the internal network
FW_DEV_INT=&quot;eth1&quot;

# Activate routing between internet and internal network
FW_ROUTE=&quot;yes&quot;

# Masquerade internal networks to the outside
FW_MASQUERADE=&quot;yes&quot;

# Interfaces to masquerade on
FW_MASQ_DEV=&quot;zone:ext&quot;

# Unrestricted access to the internet
FW_MASQ_NETS=&quot;0/0&quot;

# Any internal user can connect any service on the firewall
FW_PROTECT_FROM_INT=&quot;no&quot;

# Services on the firewall that should be accessible from untrusted networks
FW_CONFIGURATIONS_EXT=&quot;apache2 apache2-ssl bind dhcp-server sshd vsftpd xorg-x11-server&quot;

# Services accessed from the internet should be allowed to masqueraded servers (on the internal network)
FW_FORWARD_MASQ=&quot;0/0,10.1.100.123,tcp,2123,22&quot;

# Allow the firewall to reply to icmp echo requests
FW_ALLOW_PING_FW=&quot;yes&quot;</code></pre></li>
</ol>
<p>If you want a host (10.1.100.123) on the private network to be accessed publicly via SSH (port 22), you can forward the SSH request with the FW_FORWARD_MASQ option. Any request coming through port 2123 will be forwarded to the host on port 22 on the private network. It is important to note that the port number used must be free and not in use by any other service.</p>
<p>It is important to note that the interfaces that point to the internet (FW_DEV_EXT), and interfaces that point to the internal network (FW_DEV_INT) need to be set correctly. If they are not set correctly, you will have problems provisioning using xCAT.</p>
<ol start="3" style="list-style-type: decimal">
<li><p>Restart SuSEfirewall2 and load the configuration</p>
<pre><code># SuSEfirewall2 stop; SuSEfirewall2 start
SuSEfirewall2: batch committing...
SuSEfirewall2: Firewall rules unloaded.
SuSEfirewall2: Setting up rules from /etc/sysconfig/SuSEfirewall2 ...
SuSEfirewall2: batch committing...
SuSEfirewall2: Firewall rules successfully set</code></pre></li>
<li><p>Verify the NAT configuration by logging into a host (10.1.100.123) on the private network and accessing an external site from that host</p>
<pre><code># ifconfig
eth0      Link encap:Ethernet  HWaddr 02:00:01:FF:FE:FD  
          inet addr:10.1.100.123  Bcast:10.1.100.255  Mask:255.255.255.0
          inet6 addr: fd55:faaf:e1ab:263:0:6ff:feff:fefd/64 Scope:Global
          inet6 addr: fe80::6ff:feff:fefd/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1492  Metric:1
          RX packets:261657 errors:0 dropped:0 overruns:0 frame:0
          TX packets:314748 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000 
          RX bytes:103074088 (98.2 Mb)  TX bytes:27570328 (26.2 Mb)

lo        Link encap:Local Loopback  
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:16436  Metric:1
          RX packets:29 errors:0 dropped:0 overruns:0 frame:0
          TX packets:29 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0 
          RX bytes:7060 (6.8 Kb)  TX bytes:7060 (6.8 Kb)

# ping -c 4 sourceforge.net
PING sourceforge.net (216.34.181.60) 56(84) bytes of data.
64 bytes from ch3.sourceforge.net (216.34.181.60): icmp_seq=1 ttl=236 time=30.1 ms
64 bytes from ch3.sourceforge.net (216.34.181.60): icmp_seq=2 ttl=236 time=30.3 ms
64 bytes from ch3.sourceforge.net (216.34.181.60): icmp_seq=3 ttl=236 time=31.2 ms
64 bytes from ch3.sourceforge.net (216.34.181.60): icmp_seq=4 ttl=236 time=30.3 ms

--- sourceforge.net ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 3004ms
rtt min/avg/max/mdev = 30.134/30.514/31.278/0.447 ms</code></pre></li>
</ol>
<p>If the host cannot reach the external site, make sure that the gateway for the default route goes to the Linux host (10.1.100.1/gpok1) you had setup with NAT.</p>
<pre><code>    # route
    Kernel IP routing table
    Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
    10.1.100.0      *               255.255.255.0   U     0      0        0 eth0
    link-local      *               255.255.0.0     U     0      0        0 eth0
    loopback        *               255.0.0.0       U     0      0        0 lo
    default         gpok1.endicott. 0.0.0.0         UG    0      0        0 eth0</code></pre>
<p>For more information, refer to <a href="http://doc.opensuse.org/documentation/html/openSUSE/opensuse-security/cha.security.firewall.html">openSUSE Security</a>.</p>
</body>
</html>
